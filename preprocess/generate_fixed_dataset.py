"""
FIXED: Synthetic Data Generation with Proper Class Balance & Verifiable Patterns
==================================================================================
Purpose: Create a dataset where:
- Class labels are CORRECT and BALANCED
- Attack patterns are DETECTABLE (not random noise)
- Data is NORMALIZED properly
- Train/test splits are CLEAN (no leakage)
"""

import numpy as np
import json
import os
from typing import Tuple, List


def generate_realistic_normal_sample(seq_len: int, seed: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate a NORMAL (benign) sample with natural variability.
    
    Realistic pattern: Low-activity baseline with some natural fluctuations
    """
    if seed is not None:
        np.random.seed(seed)
    
    # Benign baseline: mean=0, std=0.3 (low, quiet activity)
    x_log = np.random.normal(0, 0.3, (seq_len, 32))
    x_text = np.random.normal(0, 0.3, (seq_len, 64))
    x_cve = np.random.normal(0, 0.2, (seq_len, 16))
    
    return x_log, x_text, x_cve


def generate_realistic_attack_sample(seq_len: int, seed: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Tuple[int, int]]:
    """
    Generate an ATTACK (anomalous) sample with clear, detectable patterns.
    
    Attack signature:
    - Spike in log events (higher mean)
    - Elevated text features (keywords + entropy)
    - High CVE feature activity
    - Clear temporal structure (burst in middle/end of sequence)
    """
    if seed is not None:
        np.random.seed(seed)
    
    # Start with normal baseline
    x_log = np.random.normal(0, 0.3, (seq_len, 32))
    x_text = np.random.normal(0, 0.3, (seq_len, 64))
    x_cve = np.random.normal(0, 0.2, (seq_len, 16))
    
    # Inject attack burst (multiple contiguous steps with anomalous activity)
    attack_start = np.random.randint(seq_len // 3, int(2 * seq_len / 3))
    attack_duration = np.random.randint(10, 30)  # Persisting burst lasts 10-30 timesteps
    attack_end = min(attack_start + attack_duration, seq_len)
    
    # Strong attack signal (distinct from normal)
    x_log[attack_start:attack_end] += np.random.normal(2.5, 0.4, (attack_end - attack_start, 32))
    x_text[attack_start:attack_end] += np.random.normal(2.0, 0.3, (attack_end - attack_start, 64))
    x_cve[attack_start:attack_end] += np.random.normal(1.8, 0.35, (attack_end - attack_start, 16))
    
    return x_log, x_text, x_cve, (attack_start, attack_end)


def generate_dataset(
    output_path: str,
    n_normal: int = 500,
    n_attack: int = 500,
    seq_len: int = 128,
    random_seed: int = 42,
    test_split: float = 0.2
) -> dict:
    """
    Generate a balanced, well-separated synthetic dataset.
    
    Args:
        output_path: Path to save JSON
        n_normal: Number of normal samples
        n_attack: Number of attack samples
        seq_len: Sequence length per sample
        random_seed: Global seed for reproducibility
        test_split: Fraction for test set (remaining is train)
    
    Returns:
        dict with 'train' and 'test' data and statistics
    """
    np.random.seed(random_seed)
    
    # Generate all data
    data = []
    
    # Normal samples
    for i in range(n_normal):
        x_log, x_text, x_cve = generate_realistic_normal_sample(seq_len, seed=random_seed + i)
        data.append({
            "x_log": x_log.tolist(),
            "x_text": x_text.tolist(),
            "x_cve": x_cve.tolist(),
            "label": 0,  # Normal = 0
            "attack_span": None,
        })
    
    # Attack samples
    for i in range(n_attack):
        x_log, x_text, x_cve, span = generate_realistic_attack_sample(seq_len, seed=random_seed + n_normal + i)
        data.append({
            "x_log": x_log.tolist(),
            "x_text": x_text.tolist(),
            "x_cve": x_cve.tolist(),
            "label": 1,  # Attack = 1
            "attack_span": [int(span[0]), int(span[1])],
        })
    
    # Shuffle
    np.random.RandomState(random_seed).shuffle(data)
    
    # Train/test split
    split_idx = int(len(data) * (1 - test_split))
    train_data = data[:split_idx]
    test_data = data[split_idx:]
    
    # Save both
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    base_path = output_path.replace(".json", "")
    
    with open(f"{base_path}_train.json", "w") as f:
        json.dump(train_data, f)
    
    with open(f"{base_path}_test.json", "w") as f:
        json.dump(test_data, f)
    
    # Stats
    train_labels = [x["label"] for x in train_data]
    test_labels = [x["label"] for x in test_data]
    
    stats = {
        "train_size": len(train_data),
        "test_size": len(test_data),
        "train_attack_ratio": np.mean(train_labels),
        "test_attack_ratio": np.mean(test_labels),
        "seq_len": seq_len,
        "n_features": (32, 64, 16),
        "random_seed": random_seed,
    }
    
    print(f"\nâœ“ Generated synthetic dataset")
    print(f"  Train: {stats['train_size']} samples ({stats['train_attack_ratio']:.1%} attack)")
    print(f"  Test:  {stats['test_size']} samples ({stats['test_attack_ratio']:.1%} attack)")
    print(f"  Saved to: {base_path}_train.json, {base_path}_test.json")
    
    return stats


if __name__ == "__main__":
    # Generate default dataset
    stats = generate_dataset(
        output_path="data/processed/synth_balanced",
        n_normal=500,
        n_attack=500,
        seq_len=128,
        random_seed=42,
        test_split=0.2
    )
    
    print("\nDataset Statistics:")
    for key, val in stats.items():
        print(f"  {key}: {val}")
