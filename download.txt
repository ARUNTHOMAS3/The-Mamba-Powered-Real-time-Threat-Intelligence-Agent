7.6s 1 0.00s - Debugger warning: It seems that frozen modules are being used, which may
7.6s 2 0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
7.6s 3 0.00s - to python to disable frozen modules.
7.6s 4 0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
8.2s 5 0.00s - Debugger warning: It seems that frozen modules are being used, which may
8.2s 6 0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
8.2s 7 0.00s - to python to disable frozen modules.
8.2s 8 0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
10.2s 9 /kaggle/input/datasets/kk0105/cicids2017/Week_filtered.csv
10.2s 10 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv
10.2s 11 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv
10.2s 12 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv
10.2s 13 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv
10.2s 14 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv
10.2s 15 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv
10.2s 16 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv
10.2s 17 /kaggle/input/datasets/kk0105/cicids2017/MachineLearningCSV/MachineLearningCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv
10.2s 18 Cloning into '/kaggle/working/mamba'...
10.8s 19 remote: Enumerating objects: 124, done.[K
10.8s 20 remote: Counting objects:   0% (1/124)[Kremote: Counting objects:   1% (2/124)[Kremote: Counting objects:   2% (3/124)[Kremote: Counting objects:   3% (4/124)[Kremote: Counting objects:   4% (5/124)[Kremote: Counting objects:   5% (7/124)[Kremote: Counting objects:   6% (8/124)[Kremote: Counting objects:   7% (9/124)[Kremote: Counting objects:   8% (10/124)[Kremote: Counting objects:   9% (12/124)[Kremote: Counting objects:  10% (13/124)[Kremote: Counting objects:  11% (14/124)[Kremote: Counting objects:  12% (15/124)[Kremote: Counting objects:  13% (17/124)[Kremote: Counting objects:  14% (18/124)[Kremote: Counting objects:  15% (19/124)[Kremote: Counting objects:  16% (20/124)[Kremote: Counting objects:  17% (22/124)[Kremote: Counting objects:  18% (23/124)[Kremote: Counting objects:  19% (24/124)[Kremote: Counting objects:  20% (25/124)[Kremote: Counting objects:  21% (27/124)[Kremote: Counting objects:  22% (28/124)[Kremote: Counting objects:  23% (29/124)[Kremote: Counting objects:  24% (30/124)[Kremote: Counting objects:  25% (31/124)[Kremote: Counting objects:  26% (33/124)[Kremote: Counting objects:  27% (34/124)[Kremote: Counting objects:  28% (35/124)[Kremote: Counting objects:  29% (36/124)[Kremote: Counting objects:  30% (38/124)[Kremote: Counting objects:  31% (39/124)[Kremote: Counting objects:  32% (40/124)[Kremote: Counting objects:  33% (41/124)[Kremote: Counting objects:  34% (43/124)[Kremote: Counting objects:  35% (44/124)[Kremote: Counting objects:  36% (45/124)[Kremote: Counting objects:  37% (46/124)[Kremote: Counting objects:  38% (48/124)[Kremote: Counting objects:  39% (49/124)[Kremote: Counting objects:  40% (50/124)[Kremote: Counting objects:  41% (51/124)[Kremote: Counting objects:  42% (53/124)[Kremote: Counting objects:  43% (54/124)[Kremote: Counting objects:  44% (55/124)[Kremote: Counting objects:  45% (56/124)[Kremote: Counting objects:  46% (58/124)[Kremote: Counting objects:  47% (59/124)[Kremote: Counting objects:  48% (60/124)[Kremote: Counting objects:  49% (61/124)[Kremote: Counting objects:  50% (62/124)[Kremote: Counting objects:  51% (64/124)[Kremote: Counting objects:  52% (65/124)[Kremote: Counting objects:  53% (66/124)[Kremote: Counting objects:  54% (67/124)[Kremote: Counting objects:  55% (69/124)[Kremote: Counting objects:  56% (70/124)[Kremote: Counting objects:  57% (71/124)[Kremote: Counting objects:  58% (72/124)[Kremote: Counting objects:  59% (74/124)[Kremote: Counting objects:  60% (75/124)[Kremote: Counting objects:  61% (76/124)[Kremote: Counting objects:  62% (77/124)[Kremote: Counting objects:  63% (79/124)[Kremote: Counting objects:  64% (80/124)[Kremote: Counting objects:  65% (81/124)[Kremote: Counting objects:  66% (82/124)[Kremote: Counting objects:  67% (84/124)[Kremote: Counting objects:  68% (85/124)[Kremote: Counting objects:  69% (86/124)[Kremote: Counting objects:  70% (87/124)[Kremote: Counting objects:  71% (89/124)[Kremote: Counting objects:  72% (90/124)[Kremote: Counting objects:  73% (91/124)[Kremote: Counting objects:  74% (92/124)[Kremote: Counting objects:  75% (93/124)[Kremote: Counting objects:  76% (95/124)[Kremote: Counting objects:  77% (96/124)[Kremote: Counting objects:  78% (97/124)[Kremote: Counting objects:  79% (98/124)[Kremote: Counting objects:  80% (100/124)[Kremote: Counting objects:  81% (101/124)[Kremote: Counting objects:  82% (102/124)[Kremote: Counting objects:  83% (103/124)[Kremote: Counting objects:  84% (105/124)[Kremote: Counting objects:  85% (106/124)[Kremote: Counting objects:  86% (107/124)[Kremote: Counting objects:  87% (108/124)[Kremote: Counting objects:  88% (110/124)[Kremote: Counting objects:  89% (111/124)[Kremote: Counting objects:  90% (112/124)[Kremote: Counting objects:  91% (113/124)[Kremote: Counting objects:  92% (115/124)[Kremote: Counting objects:  93% (116/124)[Kremote: Counting objects:  94% (117/124)[Kremote: Counting objects:  95% (118/124)[Kremote: Counting objects:  96% (120/124)[Kremote: Counting objects:  97% (121/124)[Kremote: Counting objects:  98% (122/124)[Kremote: Counting objects:  99% (123/124)[Kremote: Counting objects: 100% (124/124)[Kremote: Counting objects: 100% (124/124), done.[K
10.8s 21 remote: Compressing objects:   1% (1/94)[Kremote: Compressing objects:   2% (2/94)[Kremote: Compressing objects:   3% (3/94)[Kremote: Compressing objects:   4% (4/94)[Kremote: Compressing objects:   5% (5/94)[Kremote: Compressing objects:   6% (6/94)[Kremote: Compressing objects:   7% (7/94)[Kremote: Compressing objects:   8% (8/94)[Kremote: Compressing objects:   9% (9/94)[Kremote: Compressing objects:  10% (10/94)[Kremote: Compressing objects:  11% (11/94)[Kremote: Compressing objects:  12% (12/94)[Kremote: Compressing objects:  13% (13/94)[Kremote: Compressing objects:  14% (14/94)[Kremote: Compressing objects:  15% (15/94)[Kremote: Compressing objects:  17% (16/94)[Kremote: Compressing objects:  18% (17/94)[Kremote: Compressing objects:  19% (18/94)[Kremote: Compressing objects:  20% (19/94)[Kremote: Compressing objects:  21% (20/94)[Kremote: Compressing objects:  22% (21/94)[Kremote: Compressing objects:  23% (22/94)[Kremote: Compressing objects:  24% (23/94)[Kremote: Compressing objects:  25% (24/94)[Kremote: Compressing objects:  26% (25/94)[Kremote: Compressing objects:  27% (26/94)[Kremote: Compressing objects:  28% (27/94)[Kremote: Compressing objects:  29% (28/94)[Kremote: Compressing objects:  30% (29/94)[Kremote: Compressing objects:  31% (30/94)[Kremote: Compressing objects:  32% (31/94)[Kremote: Compressing objects:  34% (32/94)[Kremote: Compressing objects:  35% (33/94)[Kremote: Compressing objects:  36% (34/94)[Kremote: Compressing objects:  37% (35/94)[Kremote: Compressing objects:  38% (36/94)[Kremote: Compressing objects:  39% (37/94)[Kremote: Compressing objects:  40% (38/94)[Kremote: Compressing objects:  41% (39/94)[Kremote: Compressing objects:  42% (40/94)[Kremote: Compressing objects:  43% (41/94)[Kremote: Compressing objects:  44% (42/94)[Kremote: Compressing objects:  45% (43/94)[Kremote: Compressing objects:  46% (44/94)[Kremote: Compressing objects:  47% (45/94)[Kremote: Compressing objects:  48% (46/94)[Kremote: Compressing objects:  50% (47/94)[Kremote: Compressing objects:  51% (48/94)[Kremote: Compressing objects:  52% (49/94)[Kremote: Compressing objects:  53% (50/94)[Kremote: Compressing objects:  54% (51/94)[Kremote: Compressing objects:  55% (52/94)[Kremote: Compressing objects:  56% (53/94)[Kremote: Compressing objects:  57% (54/94)[Kremote: Compressing objects:  58% (55/94)[Kremote: Compressing objects:  59% (56/94)[Kremote: Compressing objects:  60% (57/94)[Kremote: Compressing objects:  61% (58/94)[Kremote: Compressing objects:  62% (59/94)[Kremote: Compressing objects:  63% (60/94)[Kremote: Compressing objects:  64% (61/94)[Kremote: Compressing objects:  65% (62/94)[Kremote: Compressing objects:  67% (63/94)[Kremote: Compressing objects:  68% (64/94)[Kremote: Compressing objects:  69% (65/94)[Kremote: Compressing objects:  70% (66/94)[Kremote: Compressing objects:  71% (67/94)[Kremote: Compressing objects:  72% (68/94)[Kremote: Compressing objects:  73% (69/94)[Kremote: Compressing objects:  74% (70/94)[Kremote: Compressing objects:  75% (71/94)[Kremote: Compressing objects:  76% (72/94)[Kremote: Compressing objects:  77% (73/94)[Kremote: Compressing objects:  78% (74/94)[Kremote: Compressing objects:  79% (75/94)[Kremote: Compressing objects:  80% (76/94)[Kremote: Compressing objects:  81% (77/94)[Kremote: Compressing objects:  82% (78/94)[Kremote: Compressing objects:  84% (79/94)[Kremote: Compressing objects:  85% (80/94)[Kremote: Compressing objects:  86% (81/94)[Kremote: Compressing objects:  87% (82/94)[Kremote: Compressing objects:  88% (83/94)[Kremote: Compressing objects:  89% (84/94)[Kremote: Compressing objects:  90% (85/94)[Kremote: Compressing objects:  91% (86/94)[Kremote: Compressing objects:  92% (87/94)[Kremote: Compressing objects:  93% (88/94)[Kremote: Compressing objects:  94% (89/94)[Kremote: Compressing objects:  95% (90/94)[Kremote: Compressing objects:  96% (91/94)[Kremote: Compressing objects:  97% (92/94)[Kremote: Compressing objects:  98% (93/94)[Kremote: Compressing objects: 100% (94/94)[Kremote: Compressing objects: 100% (94/94), done.[K
10.8s 22 Receiving objects:   0% (1/124)Receiving objects:   1% (2/124)Receiving objects:   2% (3/124)Receiving objects:   3% (4/124)Receiving objects:   4% (5/124)Receiving objects:   5% (7/124)Receiving objects:   6% (8/124)Receiving objects:   7% (9/124)Receiving objects:   8% (10/124)Receiving objects:   9% (12/124)Receiving objects:  10% (13/124)Receiving objects:  11% (14/124)Receiving objects:  12% (15/124)Receiving objects:  13% (17/124)Receiving objects:  14% (18/124)Receiving objects:  15% (19/124)Receiving objects:  16% (20/124)Receiving objects:  17% (22/124)Receiving objects:  18% (23/124)Receiving objects:  19% (24/124)Receiving objects:  20% (25/124)Receiving objects:  21% (27/124)Receiving objects:  22% (28/124)Receiving objects:  23% (29/124)Receiving objects:  24% (30/124)Receiving objects:  25% (31/124)Receiving objects:  26% (33/124)Receiving objects:  27% (34/124)Receiving objects:  28% (35/124)Receiving objects:  29% (36/124)Receiving objects:  30% (38/124)Receiving objects:  31% (39/124)Receiving objects:  32% (40/124)Receiving objects:  33% (41/124)Receiving objects:  34% (43/124)Receiving objects:  35% (44/124)Receiving objects:  36% (45/124)Receiving objects:  37% (46/124)Receiving objects:  38% (48/124)Receiving objects:  39% (49/124)Receiving objects:  40% (50/124)Receiving objects:  41% (51/124)Receiving objects:  42% (53/124)Receiving objects:  43% (54/124)Receiving objects:  44% (55/124)Receiving objects:  45% (56/124)Receiving objects:  46% (58/124)Receiving objects:  47% (59/124)Receiving objects:  48% (60/124)Receiving objects:  49% (61/124)Receiving objects:  50% (62/124)Receiving objects:  51% (64/124)Receiving objects:  52% (65/124)Receiving objects:  53% (66/124)Receiving objects:  54% (67/124)Receiving objects:  55% (69/124)Receiving objects:  56% (70/124)Receiving objects:  57% (71/124)Receiving objects:  58% (72/124)Receiving objects:  59% (74/124)Receiving objects:  60% (75/124)Receiving objects:  61% (76/124)Receiving objects:  62% (77/124)Receiving objects:  63% (79/124)Receiving objects:  64% (80/124)Receiving objects:  65% (81/124)Receiving objects:  66% (82/124)Receiving objects:  67% (84/124)Receiving objects:  68% (85/124)Receiving objects:  69% (86/124)Receiving objects:  70% (87/124)Receiving objects:  71% (89/124)Receiving objects:  72% (90/124)Receiving objects:  73% (91/124)Receiving objects:  74% (92/124)Receiving objects:  75% (93/124)remote: Total 124 (delta 26), reused 119 (delta 21), pack-reused 0 (from 0)[K
10.8s 23 Receiving objects:  76% (95/124)Receiving objects:  77% (96/124)Receiving objects:  78% (97/124)Receiving objects:  79% (98/124)Receiving objects:  80% (100/124)Receiving objects:  81% (101/124)Receiving objects:  82% (102/124)Receiving objects:  83% (103/124)Receiving objects:  84% (105/124)Receiving objects:  85% (106/124)Receiving objects:  86% (107/124)Receiving objects:  87% (108/124)Receiving objects:  88% (110/124)Receiving objects:  89% (111/124)Receiving objects:  90% (112/124)Receiving objects:  91% (113/124)Receiving objects:  92% (115/124)Receiving objects:  93% (116/124)Receiving objects:  94% (117/124)Receiving objects:  95% (118/124)Receiving objects:  96% (120/124)Receiving objects:  97% (121/124)Receiving objects:  98% (122/124)Receiving objects:  99% (123/124)Receiving objects: 100% (124/124)Receiving objects: 100% (124/124), 148.73 KiB | 2.86 MiB/s, done.
10.8s 24 Resolving deltas:   0% (0/26)Resolving deltas:   3% (1/26)Resolving deltas:   7% (2/26)Resolving deltas:  11% (3/26)Resolving deltas:  15% (4/26)Resolving deltas:  19% (5/26)Resolving deltas:  23% (6/26)Resolving deltas:  26% (7/26)Resolving deltas:  30% (8/26)Resolving deltas:  34% (9/26)Resolving deltas:  38% (10/26)Resolving deltas:  42% (11/26)Resolving deltas:  46% (12/26)Resolving deltas:  50% (13/26)Resolving deltas:  53% (14/26)Resolving deltas:  57% (15/26)Resolving deltas:  61% (16/26)Resolving deltas:  65% (17/26)Resolving deltas:  69% (18/26)Resolving deltas:  73% (19/26)Resolving deltas:  76% (20/26)Resolving deltas:  80% (21/26)Resolving deltas:  84% (22/26)Resolving deltas:  88% (23/26)Resolving deltas:  92% (24/26)Resolving deltas:  96% (25/26)Resolving deltas: 100% (26/26)Resolving deltas: 100% (26/26), done.
11.0s 25 /kaggle/working/mamba
11.2s 26 Already up to date.
24.1s 27 ðŸ” Detected 2 GPU(s):
24.3s 28 GPU 0: Tesla T4 (15.6 GB)
24.3s 29 GPU 1: Tesla T4 (15.6 GB)
24.3s 30 
24.3s 31 ðŸš€ Running on 2 GPUs in parallel!
24.3s 32 GPU 0: Seeds [42, 123, 456]
24.3s 33 GPU 1: Seeds [789, 1024]
24.3s 34 ============================================================
24.3s 35 âœ… GPU 0 started (PID 77) â€” seeds [42, 123, 456]
24.3s 36 âœ… GPU 1 started (PID 78) â€” seeds [789, 1024]
24.3s 37 
24.3s 38 â³ Both GPUs training in parallel â€” live output below:
24.3s 39 
28.0s 40 [GPU 0] ======================================================================
28.0s 41 [GPU 0] COMPREHENSIVE IDS BENCHMARK
28.0s 42 [GPU 0] ======================================================================
28.0s 43 [GPU 0] Datasets:  ['CICIDS2017']
28.0s 44 [GPU 0] Models:    ['Mamba', 'LSTM', 'GRU', 'Transformer', 'CNN-LSTM', 'TCN']
28.0s 45 [GPU 0] Seeds:     [42, 123, 456]
28.0s 46 [GPU 0] Device:    cuda
28.0s 47 [GPU 0] Epochs:    30
28.0s 48 [GPU 0] Batch:     128
28.0s 49 [GPU 0] Seq Len:   50
28.0s 50 [GPU 0] d_model:   128
28.0s 51 [GPU 0] n_layers:  2
28.0s 52 [GPU 0] ======================================================================
28.0s 53 [GPU 0]
28.0s 54 [GPU 0] [1/18] Running experiment...
28.0s 55 [GPU 0]
28.0s 56 [GPU 0] ======================================================================
28.0s 57 [GPU 0]   Dataset: CICIDS2017 | Model: Mamba | Seed: 42
28.0s 58 [GPU 0] ======================================================================
28.0s 59 [GPU 0] [Reproducibility] Setting all random seeds to 42
28.0s 60 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
28.0s 61 [GPU 0] [1/5] Loading CICIDS2017 dataset...
28.0s 62 [GPU 0] [TRAIN] No cache found, parsing CSVs (this only happens once)...
28.0s 63 [GPU 0] [TRAIN] Loading CICIDS2017 (4 files)...
28.0s 64 [GPU 0] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
28.0s 65 [GPU 0]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
28.0s 66 [GPU 1] ======================================================================
28.0s 67 [GPU 1] COMPREHENSIVE IDS BENCHMARK
28.0s 68 [GPU 1] ======================================================================
28.0s 69 [GPU 1] Datasets:  ['CICIDS2017']
28.0s 70 [GPU 1] Models:    ['Mamba', 'LSTM', 'GRU', 'Transformer', 'CNN-LSTM', 'TCN']
28.0s 71 [GPU 1] Seeds:     [789, 1024]
28.0s 72 [GPU 1] Device:    cuda
28.0s 73 [GPU 1] Epochs:    30
28.0s 74 [GPU 1] Batch:     128
28.0s 75 [GPU 1] Seq Len:   50
28.0s 76 [GPU 1] d_model:   128
28.0s 77 [GPU 1] n_layers:  2
28.0s 78 [GPU 1] ======================================================================
28.0s 79 [GPU 1]
28.0s 80 [GPU 1] [1/12] Running experiment...
28.0s 81 [GPU 1]
28.0s 82 [GPU 1] ======================================================================
28.0s 83 [GPU 1]   Dataset: CICIDS2017 | Model: Mamba | Seed: 789
28.0s 84 [GPU 1] ======================================================================
28.0s 85 [GPU 1] [Reproducibility] Setting all random seeds to 789
28.0s 86 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
28.0s 87 [GPU 1] [1/5] Loading CICIDS2017 dataset...
28.0s 88 [GPU 1] [TRAIN] No cache found, parsing CSVs (this only happens once)...
28.0s 89 [GPU 1] [TRAIN] Loading CICIDS2017 (4 files)...
28.0s 90 [GPU 1] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
28.0s 91 [GPU 1]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
33.5s 92 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
33.9s 93 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
35.8s 94 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
36.2s 95 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
38.4s 96 [GPU 0]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
38.7s 97 [GPU 1]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
40.4s 98 [GPU 0] Concatenating chunks...
40.7s 99 [GPU 0] Loaded 1233163 total rows.
40.7s 100 [GPU 0] Processing Labels...
40.7s 101 [GPU 1] Concatenating chunks...
40.9s 102 [GPU 1] Loaded 1233163 total rows.
40.9s 103 [GPU 1] Processing Labels...
41.2s 104 [GPU 0] Class Distribution: Benign=944240, Attack=288923
41.5s 105 [GPU 1] Class Distribution: Benign=944240, Attack=288923
41.5s 106 [GPU 0] Converting to numpy arrays...
41.7s 107 [GPU 0] Cleaning NaN/Inf values...
41.7s 108 [GPU 1] Converting to numpy arrays...
41.9s 109 [GPU 0] Total samples: 1233163
41.9s 110 [GPU 0] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
42.0s 111 [GPU 1] Cleaning NaN/Inf values...
42.0s 112 [GPU 0] Saving cache to outputs/cache/cicids2017_train.npz...
42.1s 113 [GPU 1] Total samples: 1233163
42.1s 114 [GPU 1] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
42.2s 115 [GPU 1] Saving cache to outputs/cache/cicids2017_train.npz...
51.8s 116 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
51.8s 117 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
51.8s 118 [GPU 0] Fitting StandardScaler on Train split...
51.9s 119 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
51.9s 120 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
51.9s 121 [GPU 1] Fitting StandardScaler on Train split...
52.6s 122 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
52.6s 123 [GPU 0] [OK] Will generate 863165 windows lazily during training
52.6s 124 [GPU 0] [VAL] No cache found, parsing CSVs (this only happens once)...
52.6s 125 [GPU 0] [VAL] Loading CICIDS2017 (4 files)...
52.6s 126 [GPU 0] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
52.6s 127 [GPU 0]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
52.6s 128 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
52.6s 129 [GPU 1] [OK] Will generate 863165 windows lazily during training
52.6s 130 [GPU 1] [VAL] No cache found, parsing CSVs (this only happens once)...
52.6s 131 [GPU 1] [VAL] Loading CICIDS2017 (4 files)...
52.6s 132 [GPU 1] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
52.6s 133 [GPU 1]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
57.7s 134 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
58.2s 135 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
60.0s 136 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
60.5s 137 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
62.6s 138 [GPU 0]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
63.0s 139 [GPU 1]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
64.4s 140 [GPU 0] Concatenating chunks...
64.7s 141 [GPU 0] Loaded 1233163 total rows.
64.7s 142 [GPU 0] Processing Labels...
64.9s 143 [GPU 1] Concatenating chunks...
65.1s 144 [GPU 1] Loaded 1233163 total rows.
65.1s 145 [GPU 1] Processing Labels...
65.2s 146 [GPU 0] Class Distribution: Benign=944240, Attack=288923
65.5s 147 [GPU 0] Converting to numpy arrays...
65.7s 148 [GPU 1] Class Distribution: Benign=944240, Attack=288923
65.7s 149 [GPU 0] Cleaning NaN/Inf values...
65.9s 150 [GPU 0] Total samples: 1233163
65.9s 151 [GPU 0] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
65.9s 152 [GPU 1] Converting to numpy arrays...
66.0s 153 [GPU 0] Saving cache to outputs/cache/cicids2017_val.npz...
66.2s 154 [GPU 1] Cleaning NaN/Inf values...
66.4s 155 [GPU 1] Total samples: 1233163
66.4s 156 [GPU 1] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
66.4s 157 [GPU 1] Saving cache to outputs/cache/cicids2017_val.npz...
66.5s 158 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
66.5s 159 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
66.5s 160 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
66.5s 161 [GPU 0] [OK] Will generate 123267 windows lazily during training
66.5s 162 [GPU 0] [TEST] No cache found, parsing CSVs (this only happens once)...
66.5s 163 [GPU 0] [TEST] Loading CICIDS2017 (4 files)...
66.5s 164 [GPU 0] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
66.5s 165 [GPU 0]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
66.9s 166 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
66.9s 167 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
66.9s 168 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
66.9s 169 [GPU 1] [OK] Will generate 123267 windows lazily during training
66.9s 170 [GPU 1] [TEST] No cache found, parsing CSVs (this only happens once)...
66.9s 171 [GPU 1] [TEST] Loading CICIDS2017 (4 files)...
66.9s 172 [GPU 1] Files: ['Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv']
66.9s 173 [GPU 1]  -> Processing Monday-WorkingHours.pcap_ISCX.csv...
71.6s 174 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
72.1s 175 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...
73.8s 176 [GPU 0]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
74.4s 177 [GPU 1]  -> Processing Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...
76.4s 178 [GPU 0]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
76.7s 179 [GPU 1]  -> Processing Friday-WorkingHours-Morning.pcap_ISCX.csv...
78.3s 180 [GPU 0] Concatenating chunks...
78.6s 181 [GPU 0] Loaded 1233163 total rows.
78.6s 182 [GPU 0] Processing Labels...
78.7s 183 [GPU 1] Concatenating chunks...
78.9s 184 [GPU 1] Loaded 1233163 total rows.
78.9s 185 [GPU 1] Processing Labels...
79.1s 186 [GPU 0] Class Distribution: Benign=944240, Attack=288923
79.4s 187 [GPU 0] Converting to numpy arrays...
79.4s 188 [GPU 1] Class Distribution: Benign=944240, Attack=288923
79.7s 189 [GPU 0] Cleaning NaN/Inf values...
79.7s 190 [GPU 1] Converting to numpy arrays...
79.9s 191 [GPU 0] Total samples: 1233163
79.9s 192 [GPU 0] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
79.9s 193 [GPU 0] Saving cache to outputs/cache/cicids2017_test.npz...
80.0s 194 [GPU 1] Cleaning NaN/Inf values...
80.2s 195 [GPU 1] Total samples: 1233163
80.2s 196 [GPU 1] Train: 0-863214, Val: 863214-986530, Test: 986530-1233163
80.2s 197 [GPU 1] Saving cache to outputs/cache/cicids2017_test.npz...
82.7s 198 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
82.7s 199 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
82.7s 200 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
82.8s 201 [GPU 0] [OK] Will generate 246584 windows lazily during training
82.8s 202 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
82.8s 203 [GPU 0] [2/5] Initializing Mamba (d_model=128, n_layers=2)...
82.9s 204 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
82.9s 205 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
82.9s 206 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
83.0s 207 [GPU 1] [OK] Will generate 246584 windows lazily during training
83.1s 208 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
83.1s 209 [GPU 1] [2/5] Initializing Mamba (d_model=128, n_layers=2)...
83.2s 210 [GPU 0]   Parameters: 387,073
83.3s 211 [GPU 0] [3/5] Training Mamba...
83.3s 212 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Mamba), grad_clip=0.5
83.3s 213 [GPU 1]   Parameters: 387,073
83.4s 214 [GPU 1] [3/5] Training Mamba...
83.4s 215 [GPU 1]   LR: 0.000300 (base=0.001 Ã— 0.3 for Mamba), grad_clip=0.5
88.3s 216 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
88.3s 217 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
750.8s 218 [GPU 0]   Epoch   1/30 | TrLoss: 0.0239 | VlLoss: 0.0402 | VlAcc: 0.9931 | VlF1: 0.9964 *
770.0s 219 [GPU 1]   Epoch   1/30 | TrLoss: 0.0239 | VlLoss: 0.0308 | VlAcc: 0.9954 | VlF1: 0.9976 *
1415.4s 220 [GPU 0]   Epoch   2/30 | TrLoss: 0.0354 | VlLoss: 0.0902 | VlAcc: 0.9879 | VlF1: 0.9938
1456.7s 221 [GPU 1]   Epoch   2/30 | TrLoss: 0.0083 | VlLoss: 0.0241 | VlAcc: 0.9957 | VlF1: 0.9978 *
2083.2s 222 [GPU 0]   Epoch   3/30 | TrLoss: 0.0372 | VlLoss: 0.1834 | VlAcc: 0.9642 | VlF1: 0.9818
2143.4s 223 [GPU 1]   Epoch   3/30 | TrLoss: 0.0452 | VlLoss: 0.0237 | VlAcc: 0.9938 | VlF1: 0.9968
2747.5s 224 [GPU 0]   Epoch   4/30 | TrLoss: 0.0365 | VlLoss: 0.0229 | VlAcc: 0.9968 | VlF1: 0.9983 *
2826.8s 225 [GPU 1]   Epoch   4/30 | TrLoss: 0.0345 | VlLoss: 0.0190 | VlAcc: 0.9952 | VlF1: 0.9975
3409.6s 226 [GPU 0]   Epoch   5/30 | TrLoss: 0.0228 | VlLoss: 0.0531 | VlAcc: 0.9907 | VlF1: 0.9952
3510.8s 227 [GPU 1]   Epoch   5/30 | TrLoss: 0.0041 | VlLoss: 2.5060 | VlAcc: 0.4176 | VlF1: 0.5674
4071.3s 228 [GPU 0]   Epoch   6/30 | TrLoss: 0.0088 | VlLoss: 0.0141 | VlAcc: 0.9961 | VlF1: 0.9980
4193.4s 229 [GPU 1]   Epoch   6/30 | TrLoss: 0.0025 | VlLoss: 0.8371 | VlAcc: 0.4966 | VlF1: 0.6468
4729.1s 230 [GPU 0]   Epoch   7/30 | TrLoss: 0.0027 | VlLoss: 0.0624 | VlAcc: 0.9827 | VlF1: 0.9909
4873.8s 231 [GPU 1]   Epoch   7/30 | TrLoss: 0.0019 | VlLoss: 0.6769 | VlAcc: 0.5315 | VlF1: 0.6792
4873.8s 232 [GPU 1]   Early stopping at epoch 7 (patience=5)
4873.8s 233 [GPU 1]   Best Val F1: 0.9978 (7 epochs, 4785.6s)
4873.9s 234 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Mamba_seed789.pt
4873.9s 235 [GPU 1] [4/5] Evaluating on test set...
4918.7s 236 [GPU 1]   Test Acc: 0.9699 | Prec: 0.7796 | Rec: 0.9240 | F1: 0.8457 | AUC: 0.9807
4918.7s 237 [GPU 1] [5/5] Measuring efficiency...
4921.8s 238 [GPU 1]   Latency: 14.17ms | Throughput: 2116/s | Memory: 1.48MB
4966.7s 239 [GPU 1]   Attack types analyzed: 3
4966.7s 240 [GPU 1]
4966.7s 241 [GPU 1] [2/12] Running experiment...
4966.7s 242 [GPU 1]
4966.7s 243 [GPU 1] ======================================================================
4966.7s 244 [GPU 1]   Dataset: CICIDS2017 | Model: LSTM | Seed: 789
4966.7s 245 [GPU 1] ======================================================================
4966.7s 246 [GPU 1] [Reproducibility] Setting all random seeds to 789
4966.7s 247 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
4966.7s 248 [GPU 1] [1/5] Loading CICIDS2017 dataset...
4966.7s 249 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
4967.8s 250 [GPU 1]   Cached shape: (863214, 77), labels: 863214
4967.8s 251 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
4967.8s 252 [GPU 1] Fitting StandardScaler on Train split...
4968.7s 253 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
4968.7s 254 [GPU 1] [OK] Will generate 863165 windows lazily during training
4968.7s 255 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
4968.8s 256 [GPU 1]   Cached shape: (123316, 77), labels: 123316
4968.8s 257 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
4968.8s 258 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
4968.8s 259 [GPU 1] [OK] Will generate 123267 windows lazily during training
4968.8s 260 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
4969.1s 261 [GPU 1]   Cached shape: (246633, 77), labels: 246633
4969.1s 262 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
4969.1s 263 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
4969.2s 264 [GPU 1] [OK] Will generate 246584 windows lazily during training
4969.2s 265 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
4969.2s 266 [GPU 1] [2/5] Initializing LSTM (d_model=128, n_layers=2)...
4969.3s 267 [GPU 1]   Parameters: 423,169
4969.4s 268 [GPU 1] [3/5] Training LSTM...
4969.4s 269 [GPU 1]   LR: 0.001000 (base=0.001 Ã— 1.0 for LSTM), grad_clip=1.0
4969.4s 270 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
5061.4s 271 [GPU 1]   Epoch   1/30 | TrLoss: 0.0409 | VlLoss: 0.1850 | VlAcc: 0.9642 | VlF1: 0.9818 *
5153.6s 272 [GPU 1]   Epoch   2/30 | TrLoss: 0.0327 | VlLoss: 0.1807 | VlAcc: 0.9642 | VlF1: 0.9818
5245.8s 273 [GPU 1]   Epoch   3/30 | TrLoss: 0.0337 | VlLoss: 0.1809 | VlAcc: 0.9642 | VlF1: 0.9818
5337.8s 274 [GPU 1]   Epoch   4/30 | TrLoss: 0.0345 | VlLoss: 0.1798 | VlAcc: 0.9642 | VlF1: 0.9818
5387.2s 275 [GPU 0]   Epoch   8/30 | TrLoss: 0.0018 | VlLoss: 0.0614 | VlAcc: 0.9775 | VlF1: 0.9882
5429.7s 276 [GPU 1]   Epoch   5/30 | TrLoss: 0.0443 | VlLoss: 0.1840 | VlAcc: 0.9642 | VlF1: 0.9818
5521.9s 277 [GPU 1]   Epoch   6/30 | TrLoss: 0.0309 | VlLoss: 0.1132 | VlAcc: 0.9642 | VlF1: 0.9818
5521.9s 278 [GPU 1]   Early stopping at epoch 6 (patience=5)
5522.0s 279 [GPU 1]   Best Val F1: 0.9818 (6 epochs, 552.6s)
5522.0s 280 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_LSTM_seed789.pt
5522.0s 281 [GPU 1] [4/5] Evaluating on test set...
5534.6s 282 [GPU 1]   Test Acc: 0.0893 | Prec: 0.0893 | Rec: 1.0000 | F1: 0.1640 | AUC: 0.6837
5534.6s 283 [GPU 1] [5/5] Measuring efficiency...
5534.8s 284 [GPU 1]   Latency: 0.75ms | Throughput: 28172/s | Memory: 1.61MB
5547.5s 285 [GPU 1]   Attack types analyzed: 3
5547.5s 286 [GPU 1]
5547.5s 287 [GPU 1] [3/12] Running experiment...
5547.5s 288 [GPU 1]
5547.5s 289 [GPU 1] ======================================================================
5547.5s 290 [GPU 1]   Dataset: CICIDS2017 | Model: GRU | Seed: 789
5547.5s 291 [GPU 1] ======================================================================
5547.5s 292 [GPU 1] [Reproducibility] Setting all random seeds to 789
5547.5s 293 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
5547.5s 294 [GPU 1] [1/5] Loading CICIDS2017 dataset...
5547.5s 295 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
5548.5s 296 [GPU 1]   Cached shape: (863214, 77), labels: 863214
5548.5s 297 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
5548.5s 298 [GPU 1] Fitting StandardScaler on Train split...
5549.4s 299 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
5549.4s 300 [GPU 1] [OK] Will generate 863165 windows lazily during training
5549.4s 301 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
5549.5s 302 [GPU 1]   Cached shape: (123316, 77), labels: 123316
5549.5s 303 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
5549.5s 304 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
5549.6s 305 [GPU 1] [OK] Will generate 123267 windows lazily during training
5549.6s 306 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
5549.9s 307 [GPU 1]   Cached shape: (246633, 77), labels: 246633
5549.9s 308 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
5549.9s 309 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
5549.9s 310 [GPU 1] [OK] Will generate 246584 windows lazily during training
5549.9s 311 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
5549.9s 312 [GPU 1] [2/5] Initializing GRU (d_model=128, n_layers=2)...
5550.0s 313 [GPU 1]   Parameters: 373,505
5550.1s 314 [GPU 1] [3/5] Training GRU...
5550.1s 315 [GPU 1]   LR: 0.001000 (base=0.001 Ã— 1.0 for GRU), grad_clip=1.0
5550.1s 316 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
5624.5s 317 [GPU 1]   Epoch   1/30 | TrLoss: 0.0531 | VlLoss: 0.1852 | VlAcc: 0.9642 | VlF1: 0.9818 *
5698.3s 318 [GPU 1]   Epoch   2/30 | TrLoss: 0.1224 | VlLoss: 0.1830 | VlAcc: 0.9642 | VlF1: 0.9818
5772.8s 319 [GPU 1]   Epoch   3/30 | TrLoss: 0.0460 | VlLoss: 0.1778 | VlAcc: 0.9642 | VlF1: 0.9818
5847.0s 320 [GPU 1]   Epoch   4/30 | TrLoss: 0.0323 | VlLoss: 0.1788 | VlAcc: 0.9642 | VlF1: 0.9818
5921.3s 321 [GPU 1]   Epoch   5/30 | TrLoss: 0.0427 | VlLoss: 0.1822 | VlAcc: 0.9642 | VlF1: 0.9818
5995.7s 322 [GPU 1]   Epoch   6/30 | TrLoss: 0.0334 | VlLoss: 0.1831 | VlAcc: 0.9642 | VlF1: 0.9818
5995.7s 323 [GPU 1]   Early stopping at epoch 6 (patience=5)
5995.7s 324 [GPU 1]   Best Val F1: 0.9818 (6 epochs, 445.7s)
5995.7s 325 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_GRU_seed789.pt
5995.7s 326 [GPU 1] [4/5] Evaluating on test set...
6006.4s 327 [GPU 1]   Test Acc: 0.0894 | Prec: 0.0894 | Rec: 1.0000 | F1: 0.1640 | AUC: 0.7219
6006.4s 328 [GPU 1] [5/5] Measuring efficiency...
6006.6s 329 [GPU 1]   Latency: 0.50ms | Throughput: 41167/s | Memory: 1.42MB
6017.3s 330 [GPU 1]   Attack types analyzed: 3
6017.3s 331 [GPU 1]
6017.3s 332 [GPU 1] [4/12] Running experiment...
6017.3s 333 [GPU 1]
6017.3s 334 [GPU 1] ======================================================================
6017.3s 335 [GPU 1]   Dataset: CICIDS2017 | Model: Transformer | Seed: 789
6017.3s 336 [GPU 1] ======================================================================
6017.3s 337 [GPU 1] [Reproducibility] Setting all random seeds to 789
6017.3s 338 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
6017.3s 339 [GPU 1] [1/5] Loading CICIDS2017 dataset...
6017.3s 340 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
6018.4s 341 [GPU 1]   Cached shape: (863214, 77), labels: 863214
6018.4s 342 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
6018.4s 343 [GPU 1] Fitting StandardScaler on Train split...
6019.3s 344 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
6019.3s 345 [GPU 1] [OK] Will generate 863165 windows lazily during training
6019.3s 346 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
6019.4s 347 [GPU 1]   Cached shape: (123316, 77), labels: 123316
6019.4s 348 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
6019.4s 349 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
6019.4s 350 [GPU 1] [OK] Will generate 123267 windows lazily during training
6019.4s 351 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
6019.7s 352 [GPU 1]   Cached shape: (246633, 77), labels: 246633
6019.7s 353 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
6019.7s 354 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
6019.9s 355 [GPU 1] [OK] Will generate 246584 windows lazily during training
6019.9s 356 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
6019.9s 357 [GPU 1] [2/5] Initializing Transformer (d_model=128, n_layers=2)...
6019.9s 358 [GPU 1]   Parameters: 406,913
6020.0s 359 [GPU 1] [3/5] Training Transformer...
6020.0s 360 [GPU 1]   LR: 0.000300 (base=0.001 Ã— 0.3 for Transformer), grad_clip=0.5
6020.0s 361 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
6047.5s 362 [GPU 0]   Epoch   9/30 | TrLoss: 0.0018 | VlLoss: 0.0280 | VlAcc: 0.9885 | VlF1: 0.9940
6047.5s 363 [GPU 0]   Early stopping at epoch 9 (patience=5)
6047.5s 364 [GPU 0]   Best Val F1: 0.9983 (9 epochs, 5959.2s)
6047.5s 365 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Mamba_seed42.pt
6047.5s 366 [GPU 0] [4/5] Evaluating on test set...
6092.6s 367 [GPU 0]   Test Acc: 0.9726 | Prec: 0.7842 | Rec: 0.9573 | F1: 0.8621 | AUC: 0.9798
6092.6s 368 [GPU 0] [5/5] Measuring efficiency...
6096.2s 369 [GPU 0]   Latency: 16.73ms | Throughput: 1873/s | Memory: 1.48MB
6128.1s 370 [GPU 1]   Epoch   1/30 | TrLoss: 0.1020 | VlLoss: 0.1808 | VlAcc: 0.9642 | VlF1: 0.9818 *
6140.6s 371 [GPU 0]   Attack types analyzed: 3
6140.6s 372 [GPU 0]
6140.6s 373 [GPU 0] [2/18] Running experiment...
6140.6s 374 [GPU 0]
6140.6s 375 [GPU 0] ======================================================================
6140.6s 376 [GPU 0]   Dataset: CICIDS2017 | Model: LSTM | Seed: 42
6140.6s 377 [GPU 0] ======================================================================
6140.6s 378 [GPU 0] [Reproducibility] Setting all random seeds to 42
6140.6s 379 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
6140.6s 380 [GPU 0] [1/5] Loading CICIDS2017 dataset...
6140.6s 381 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
6141.8s 382 [GPU 0]   Cached shape: (863214, 77), labels: 863214
6141.8s 383 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
6141.8s 384 [GPU 0] Fitting StandardScaler on Train split...
6142.8s 385 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
6142.8s 386 [GPU 0] [OK] Will generate 863165 windows lazily during training
6142.8s 387 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
6142.9s 388 [GPU 0]   Cached shape: (123316, 77), labels: 123316
6142.9s 389 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
6142.9s 390 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
6142.9s 391 [GPU 0] [OK] Will generate 123267 windows lazily during training
6142.9s 392 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
6143.3s 393 [GPU 0]   Cached shape: (246633, 77), labels: 246633
6143.3s 394 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
6143.3s 395 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
6143.4s 396 [GPU 0] [OK] Will generate 246584 windows lazily during training
6143.4s 397 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
6143.4s 398 [GPU 0] [2/5] Initializing LSTM (d_model=128, n_layers=2)...
6143.4s 399 [GPU 0]   Parameters: 423,169
6143.5s 400 [GPU 0] [3/5] Training LSTM...
6143.5s 401 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for LSTM), grad_clip=1.0
6143.5s 402 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
6233.7s 403 [GPU 0]   Epoch   1/30 | TrLoss: 0.0367 | VlLoss: 0.1839 | VlAcc: 0.9642 | VlF1: 0.9818 *
6238.3s 404 [GPU 1]   Epoch   2/30 | TrLoss: 0.0603 | VlLoss: 0.1899 | VlAcc: 0.9642 | VlF1: 0.9818
6323.6s 405 [GPU 0]   Epoch   2/30 | TrLoss: 0.0473 | VlLoss: 0.1846 | VlAcc: 0.9642 | VlF1: 0.9818
6348.2s 406 [GPU 1]   Epoch   3/30 | TrLoss: 0.0632 | VlLoss: 0.1951 | VlAcc: 0.9642 | VlF1: 0.9818
6413.6s 407 [GPU 0]   Epoch   3/30 | TrLoss: 0.0373 | VlLoss: 0.1810 | VlAcc: 0.9642 | VlF1: 0.9818
6458.6s 408 [GPU 1]   Epoch   4/30 | TrLoss: 0.0519 | VlLoss: 0.1937 | VlAcc: 0.9642 | VlF1: 0.9818
6504.2s 409 [GPU 0]   Epoch   4/30 | TrLoss: 0.0341 | VlLoss: 0.1802 | VlAcc: 0.9642 | VlF1: 0.9818
6569.3s 410 [GPU 1]   Epoch   5/30 | TrLoss: 0.0701 | VlLoss: 0.1965 | VlAcc: 0.9642 | VlF1: 0.9818
6594.9s 411 [GPU 0]   Epoch   5/30 | TrLoss: 0.0596 | VlLoss: 0.1866 | VlAcc: 0.9642 | VlF1: 0.9818
6679.4s 412 [GPU 1]   Epoch   6/30 | TrLoss: 0.0605 | VlLoss: 0.1833 | VlAcc: 0.9642 | VlF1: 0.9818
6679.4s 413 [GPU 1]   Early stopping at epoch 6 (patience=5)
6679.4s 414 [GPU 1]   Best Val F1: 0.9818 (6 epochs, 659.4s)
6679.4s 415 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Transformer_seed789.pt
6679.4s 416 [GPU 1] [4/5] Evaluating on test set...
6684.6s 417 [GPU 0]   Epoch   6/30 | TrLoss: 0.0390 | VlLoss: 0.1840 | VlAcc: 0.9642 | VlF1: 0.9818
6684.6s 418 [GPU 0]   Early stopping at epoch 6 (patience=5)
6684.6s 419 [GPU 0]   Best Val F1: 0.9818 (6 epochs, 541.1s)
6684.6s 420 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_LSTM_seed42.pt
6684.6s 421 [GPU 0] [4/5] Evaluating on test set...
6690.6s 422 [GPU 1]   Test Acc: 0.0894 | Prec: 0.0894 | Rec: 1.0000 | F1: 0.1640 | AUC: 0.9216
6690.6s 423 [GPU 1] [5/5] Measuring efficiency...
6690.9s 424 [GPU 1]   Latency: 1.17ms | Throughput: 25918/s | Memory: 1.55MB
6697.6s 425 [GPU 0]   Test Acc: 0.0895 | Prec: 0.0894 | Rec: 1.0000 | F1: 0.1641 | AUC: 0.4995
6697.6s 426 [GPU 0] [5/5] Measuring efficiency...
6697.8s 427 [GPU 0]   Latency: 0.75ms | Throughput: 30849/s | Memory: 1.61MB
6702.1s 428 [GPU 1]   Attack types analyzed: 3
6702.1s 429 [GPU 1]
6702.1s 430 [GPU 1] [5/12] Running experiment...
6702.1s 431 [GPU 1]
6702.1s 432 [GPU 1] ======================================================================
6702.1s 433 [GPU 1]   Dataset: CICIDS2017 | Model: CNN-LSTM | Seed: 789
6702.1s 434 [GPU 1] ======================================================================
6702.1s 435 [GPU 1] [Reproducibility] Setting all random seeds to 789
6702.1s 436 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
6702.1s 437 [GPU 1] [1/5] Loading CICIDS2017 dataset...
6702.1s 438 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
6703.2s 439 [GPU 1]   Cached shape: (863214, 77), labels: 863214
6703.2s 440 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
6703.2s 441 [GPU 1] Fitting StandardScaler on Train split...
6704.2s 442 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
6704.2s 443 [GPU 1] [OK] Will generate 863165 windows lazily during training
6704.2s 444 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
6704.3s 445 [GPU 1]   Cached shape: (123316, 77), labels: 123316
6704.3s 446 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
6704.3s 447 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
6704.3s 448 [GPU 1] [OK] Will generate 123267 windows lazily during training
6704.3s 449 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
6704.6s 450 [GPU 1]   Cached shape: (246633, 77), labels: 246633
6704.6s 451 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
6704.6s 452 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
6704.7s 453 [GPU 1] [OK] Will generate 246584 windows lazily during training
6704.7s 454 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
6704.7s 455 [GPU 1] [2/5] Initializing CNN-LSTM (d_model=128, n_layers=2)...
6704.7s 456 [GPU 1]   Parameters: 409,601
6704.8s 457 [GPU 1] [3/5] Training CNN-LSTM...
6704.8s 458 [GPU 1]   LR: 0.000500 (base=0.001 Ã— 0.5 for CNN-LSTM), grad_clip=0.5
6704.8s 459 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
6710.3s 460 [GPU 0]   Attack types analyzed: 3
6710.3s 461 [GPU 0]
6710.3s 462 [GPU 0] [3/18] Running experiment...
6710.3s 463 [GPU 0]
6710.3s 464 [GPU 0] ======================================================================
6710.3s 465 [GPU 0]   Dataset: CICIDS2017 | Model: GRU | Seed: 42
6710.3s 466 [GPU 0] ======================================================================
6710.3s 467 [GPU 0] [Reproducibility] Setting all random seeds to 42
6710.3s 468 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
6710.3s 469 [GPU 0] [1/5] Loading CICIDS2017 dataset...
6710.3s 470 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
6711.4s 471 [GPU 0]   Cached shape: (863214, 77), labels: 863214
6711.4s 472 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
6711.4s 473 [GPU 0] Fitting StandardScaler on Train split...
6712.3s 474 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
6712.3s 475 [GPU 0] [OK] Will generate 863165 windows lazily during training
6712.3s 476 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
6712.4s 477 [GPU 0]   Cached shape: (123316, 77), labels: 123316
6712.4s 478 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
6712.4s 479 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
6712.5s 480 [GPU 0] [OK] Will generate 123267 windows lazily during training
6712.5s 481 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
6712.8s 482 [GPU 0]   Cached shape: (246633, 77), labels: 246633
6712.8s 483 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
6712.8s 484 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
6712.9s 485 [GPU 0] [OK] Will generate 246584 windows lazily during training
6712.9s 486 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
6712.9s 487 [GPU 0] [2/5] Initializing GRU (d_model=128, n_layers=2)...
6712.9s 488 [GPU 0]   Parameters: 373,505
6713.0s 489 [GPU 0] [3/5] Training GRU...
6713.0s 490 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for GRU), grad_clip=1.0
6713.0s 491 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
6785.3s 492 [GPU 0]   Epoch   1/30 | TrLoss: 0.0343 | VlLoss: 0.1800 | VlAcc: 0.9642 | VlF1: 0.9818 *
6798.6s 493 [GPU 1]   Epoch   1/30 | TrLoss: 0.0825 | VlLoss: 0.4052 | VlAcc: 0.9575 | VlF1: 0.9778 *
6857.5s 494 [GPU 0]   Epoch   2/30 | TrLoss: 0.0634 | VlLoss: 0.1867 | VlAcc: 0.9642 | VlF1: 0.9818
6891.8s 495 [GPU 1]   Epoch   2/30 | TrLoss: 0.0503 | VlLoss: 0.8013 | VlAcc: 0.3734 | VlF1: 0.5203
6929.7s 496 [GPU 0]   Epoch   3/30 | TrLoss: 0.0377 | VlLoss: 0.1813 | VlAcc: 0.9642 | VlF1: 0.9818
6985.0s 497 [GPU 1]   Epoch   3/30 | TrLoss: 0.0167 | VlLoss: 0.2414 | VlAcc: 0.9844 | VlF1: 0.9919 *
7001.7s 498 [GPU 0]   Epoch   4/30 | TrLoss: 0.0346 | VlLoss: 0.1825 | VlAcc: 0.9642 | VlF1: 0.9818
7073.7s 499 [GPU 0]   Epoch   5/30 | TrLoss: 0.0412 | VlLoss: 0.1497 | VlAcc: 0.9642 | VlF1: 0.9818
7078.1s 500 [GPU 1]   Epoch   4/30 | TrLoss: 0.0177 | VlLoss: 1.4736 | VlAcc: 0.3572 | VlF1: 0.5018
7145.8s 501 [GPU 0]   Epoch   6/30 | TrLoss: 0.0230 | VlLoss: 0.1147 | VlAcc: 0.9642 | VlF1: 0.9818
7145.8s 502 [GPU 0]   Early stopping at epoch 6 (patience=5)
7145.8s 503 [GPU 0]   Best Val F1: 0.9818 (6 epochs, 432.9s)
7145.8s 504 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_GRU_seed42.pt
7145.8s 505 [GPU 0] [4/5] Evaluating on test set...
7156.3s 506 [GPU 0]   Test Acc: 0.0894 | Prec: 0.0893 | Rec: 1.0000 | F1: 0.1640 | AUC: 0.5190
7156.3s 507 [GPU 0] [5/5] Measuring efficiency...
7156.5s 508 [GPU 0]   Latency: 0.50ms | Throughput: 43813/s | Memory: 1.42MB
7166.9s 509 [GPU 0]   Attack types analyzed: 3
7166.9s 510 [GPU 0]
7166.9s 511 [GPU 0] [4/18] Running experiment...
7166.9s 512 [GPU 0]
7166.9s 513 [GPU 0] ======================================================================
7166.9s 514 [GPU 0]   Dataset: CICIDS2017 | Model: Transformer | Seed: 42
7166.9s 515 [GPU 0] ======================================================================
7166.9s 516 [GPU 0] [Reproducibility] Setting all random seeds to 42
7166.9s 517 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
7166.9s 518 [GPU 0] [1/5] Loading CICIDS2017 dataset...
7166.9s 519 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
7168.1s 520 [GPU 0]   Cached shape: (863214, 77), labels: 863214
7168.1s 521 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
7168.1s 522 [GPU 0] Fitting StandardScaler on Train split...
7169.0s 523 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
7169.0s 524 [GPU 0] [OK] Will generate 863165 windows lazily during training
7169.0s 525 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
7169.0s 526 [GPU 0]   Cached shape: (123316, 77), labels: 123316
7169.0s 527 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
7169.0s 528 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
7169.1s 529 [GPU 0] [OK] Will generate 123267 windows lazily during training
7169.1s 530 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
7169.4s 531 [GPU 0]   Cached shape: (246633, 77), labels: 246633
7169.4s 532 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
7169.4s 533 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
7169.5s 534 [GPU 0] [OK] Will generate 246584 windows lazily during training
7169.5s 535 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
7169.5s 536 [GPU 0] [2/5] Initializing Transformer (d_model=128, n_layers=2)...
7169.5s 537 [GPU 0]   Parameters: 406,913
7169.6s 538 [GPU 0] [3/5] Training Transformer...
7169.6s 539 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Transformer), grad_clip=0.5
7169.6s 540 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
7171.1s 541 [GPU 1]   Epoch   5/30 | TrLoss: 0.0044 | VlLoss: 0.8847 | VlAcc: 0.4480 | VlF1: 0.6004
7268.1s 542 [GPU 1]   Epoch   6/30 | TrLoss: 0.0036 | VlLoss: 0.0258 | VlAcc: 0.9942 | VlF1: 0.9970 *
7273.3s 543 [GPU 0]   Epoch   1/30 | TrLoss: 0.1064 | VlLoss: 0.1935 | VlAcc: 0.9642 | VlF1: 0.9818 *
7365.2s 544 [GPU 1]   Epoch   7/30 | TrLoss: 0.0023 | VlLoss: 0.0285 | VlAcc: 0.9941 | VlF1: 0.9970
7377.4s 545 [GPU 0]   Epoch   2/30 | TrLoss: 0.0587 | VlLoss: 0.1925 | VlAcc: 0.9642 | VlF1: 0.9818
7462.3s 546 [GPU 1]   Epoch   8/30 | TrLoss: 0.0018 | VlLoss: 0.0298 | VlAcc: 0.9944 | VlF1: 0.9971 *
7481.4s 547 [GPU 0]   Epoch   3/30 | TrLoss: 0.0501 | VlLoss: 0.1893 | VlAcc: 0.9642 | VlF1: 0.9818 *
7559.4s 548 [GPU 1]   Epoch   9/30 | TrLoss: 0.0020 | VlLoss: 0.0339 | VlAcc: 0.9946 | VlF1: 0.9972 *
7586.2s 549 [GPU 0]   Epoch   4/30 | TrLoss: 0.0450 | VlLoss: 0.1943 | VlAcc: 0.9642 | VlF1: 0.9818
7656.8s 550 [GPU 1]   Epoch  10/30 | TrLoss: 0.0018 | VlLoss: 0.0329 | VlAcc: 0.9950 | VlF1: 0.9974 *
7690.8s 551 [GPU 0]   Epoch   5/30 | TrLoss: 0.0610 | VlLoss: 0.1951 | VlAcc: 0.9642 | VlF1: 0.9818
7754.2s 552 [GPU 1]   Epoch  11/30 | TrLoss: 0.0013 | VlLoss: 0.0306 | VlAcc: 0.9949 | VlF1: 0.9974
7795.5s 553 [GPU 0]   Epoch   6/30 | TrLoss: 0.0445 | VlLoss: 0.1690 | VlAcc: 0.9636 | VlF1: 0.9815
7851.4s 554 [GPU 1]   Epoch  12/30 | TrLoss: 0.0018 | VlLoss: 0.0234 | VlAcc: 0.9965 | VlF1: 0.9982 *
7899.9s 555 [GPU 0]   Epoch   7/30 | TrLoss: 0.0381 | VlLoss: 0.7157 | VlAcc: 0.3522 | VlF1: 0.5010
7948.6s 556 [GPU 1]   Epoch  13/30 | TrLoss: 0.0015 | VlLoss: 0.0303 | VlAcc: 0.9963 | VlF1: 0.9981
8004.6s 557 [GPU 0]   Epoch   8/30 | TrLoss: 0.0422 | VlLoss: 0.2302 | VlAcc: 0.9641 | VlF1: 0.9817
8004.6s 558 [GPU 0]   Early stopping at epoch 8 (patience=5)
8004.6s 559 [GPU 0]   Best Val F1: 0.9818 (8 epochs, 835.0s)
8004.6s 560 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Transformer_seed42.pt
8004.6s 561 [GPU 0] [4/5] Evaluating on test set...
8016.4s 562 [GPU 0]   Test Acc: 0.0893 | Prec: 0.0893 | Rec: 0.9992 | F1: 0.1639 | AUC: 0.3965
8016.4s 563 [GPU 0] [5/5] Measuring efficiency...
8016.7s 564 [GPU 0]   Latency: 1.30ms | Throughput: 25978/s | Memory: 1.55MB
8028.8s 565 [GPU 0]   Attack types analyzed: 3
8028.8s 566 [GPU 0]
8028.8s 567 [GPU 0] [5/18] Running experiment...
8028.8s 568 [GPU 0]
8028.8s 569 [GPU 0] ======================================================================
8028.8s 570 [GPU 0]   Dataset: CICIDS2017 | Model: CNN-LSTM | Seed: 42
8028.8s 571 [GPU 0] ======================================================================
8028.8s 572 [GPU 0] [Reproducibility] Setting all random seeds to 42
8028.8s 573 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
8028.8s 574 [GPU 0] [1/5] Loading CICIDS2017 dataset...
8028.8s 575 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
8030.0s 576 [GPU 0]   Cached shape: (863214, 77), labels: 863214
8030.0s 577 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
8030.0s 578 [GPU 0] Fitting StandardScaler on Train split...
8030.9s 579 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
8030.9s 580 [GPU 0] [OK] Will generate 863165 windows lazily during training
8030.9s 581 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
8031.0s 582 [GPU 0]   Cached shape: (123316, 77), labels: 123316
8031.0s 583 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
8031.0s 584 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
8031.1s 585 [GPU 0] [OK] Will generate 123267 windows lazily during training
8031.1s 586 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
8031.4s 587 [GPU 0]   Cached shape: (246633, 77), labels: 246633
8031.4s 588 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
8031.4s 589 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
8031.5s 590 [GPU 0] [OK] Will generate 246584 windows lazily during training
8031.5s 591 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
8031.5s 592 [GPU 0] [2/5] Initializing CNN-LSTM (d_model=128, n_layers=2)...
8031.5s 593 [GPU 0]   Parameters: 409,601
8031.6s 594 [GPU 0] [3/5] Training CNN-LSTM...
8031.6s 595 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for CNN-LSTM), grad_clip=0.5
8031.6s 596 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
8045.5s 597 [GPU 1]   Epoch  14/30 | TrLoss: 0.0010 | VlLoss: 0.0266 | VlAcc: 0.9965 | VlF1: 0.9982 *
8119.0s 598 [GPU 0]   Epoch   1/30 | TrLoss: 0.0809 | VlLoss: 0.1745 | VlAcc: 0.9642 | VlF1: 0.9818 *
8138.3s 599 [GPU 1]   Epoch  15/30 | TrLoss: 0.0009 | VlLoss: 0.0204 | VlAcc: 0.9970 | VlF1: 0.9985 *
8206.3s 600 [GPU 0]   Epoch   2/30 | TrLoss: 0.0457 | VlLoss: 0.1207 | VlAcc: 0.9804 | VlF1: 0.9899 *
8231.1s 601 [GPU 1]   Epoch  16/30 | TrLoss: 0.0009 | VlLoss: 0.0274 | VlAcc: 0.9960 | VlF1: 0.9979
8293.6s 602 [GPU 0]   Epoch   3/30 | TrLoss: 0.0395 | VlLoss: 0.1103 | VlAcc: 0.9816 | VlF1: 0.9905 *
8323.7s 603 [GPU 1]   Epoch  17/30 | TrLoss: 0.0007 | VlLoss: 0.0324 | VlAcc: 0.9963 | VlF1: 0.9981
8381.2s 604 [GPU 0]   Epoch   4/30 | TrLoss: 0.0151 | VlLoss: 0.0507 | VlAcc: 0.9931 | VlF1: 0.9964 *
8416.4s 605 [GPU 1]   Epoch  18/30 | TrLoss: 0.0007 | VlLoss: 0.0349 | VlAcc: 0.9945 | VlF1: 0.9971
8468.6s 606 [GPU 0]   Epoch   5/30 | TrLoss: 0.0099 | VlLoss: 0.2716 | VlAcc: 0.8886 | VlF1: 0.9389
8509.0s 607 [GPU 1]   Epoch  19/30 | TrLoss: 0.0005 | VlLoss: 0.0334 | VlAcc: 0.9961 | VlF1: 0.9980
8556.7s 608 [GPU 0]   Epoch   6/30 | TrLoss: 0.0032 | VlLoss: 0.0677 | VlAcc: 0.9946 | VlF1: 0.9972 *
8601.7s 609 [GPU 1]   Epoch  20/30 | TrLoss: 0.0004 | VlLoss: 0.0333 | VlAcc: 0.9956 | VlF1: 0.9977
8601.7s 610 [GPU 1]   Early stopping at epoch 20 (patience=5)
8601.7s 611 [GPU 1]   Best Val F1: 0.9985 (20 epochs, 1896.9s)
8601.7s 612 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_CNN-LSTM_seed789.pt
8601.7s 613 [GPU 1] [4/5] Evaluating on test set...
8613.4s 614 [GPU 1]   Test Acc: 0.9729 | Prec: 0.7962 | Rec: 0.9361 | F1: 0.8605 | AUC: 0.9747
8613.4s 615 [GPU 1] [5/5] Measuring efficiency...
8613.6s 616 [GPU 1]   Latency: 0.90ms | Throughput: 31031/s | Memory: 1.56MB
8625.1s 617 [GPU 1]   Attack types analyzed: 3
8625.1s 618 [GPU 1]
8625.1s 619 [GPU 1] [6/12] Running experiment...
8625.1s 620 [GPU 1]
8625.1s 621 [GPU 1] ======================================================================
8625.1s 622 [GPU 1]   Dataset: CICIDS2017 | Model: TCN | Seed: 789
8625.1s 623 [GPU 1] ======================================================================
8625.1s 624 [GPU 1] [Reproducibility] Setting all random seeds to 789
8625.1s 625 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
8625.1s 626 [GPU 1] [1/5] Loading CICIDS2017 dataset...
8625.1s 627 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
8626.2s 628 [GPU 1]   Cached shape: (863214, 77), labels: 863214
8626.2s 629 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
8626.2s 630 [GPU 1] Fitting StandardScaler on Train split...
8627.2s 631 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
8627.2s 632 [GPU 1] [OK] Will generate 863165 windows lazily during training
8627.2s 633 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
8627.3s 634 [GPU 1]   Cached shape: (123316, 77), labels: 123316
8627.3s 635 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
8627.3s 636 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
8627.3s 637 [GPU 1] [OK] Will generate 123267 windows lazily during training
8627.4s 638 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
8627.7s 639 [GPU 1]   Cached shape: (246633, 77), labels: 246633
8627.7s 640 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
8627.7s 641 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
8627.8s 642 [GPU 1] [OK] Will generate 246584 windows lazily during training
8627.8s 643 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
8627.8s 644 [GPU 1] [2/5] Initializing TCN (d_model=128, n_layers=2)...
8627.8s 645 [GPU 1]   Parameters: 423,169
8627.9s 646 [GPU 1] [3/5] Training TCN...
8627.9s 647 [GPU 1]   LR: 0.000500 (base=0.001 Ã— 0.5 for TCN), grad_clip=1.0
8627.9s 648 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
8644.1s 649 [GPU 0]   Epoch   7/30 | TrLoss: 0.0023 | VlLoss: 0.0264 | VlAcc: 0.9948 | VlF1: 0.9973 *
8731.4s 650 [GPU 0]   Epoch   8/30 | TrLoss: 0.0020 | VlLoss: 0.0281 | VlAcc: 0.9948 | VlF1: 0.9973
8744.0s 651 [GPU 1]   Epoch   1/30 | TrLoss: 0.0382 | VlLoss: 0.0512 | VlAcc: 0.9790 | VlF1: 0.9890 *
8818.9s 652 [GPU 0]   Epoch   9/30 | TrLoss: 0.0018 | VlLoss: 0.0222 | VlAcc: 0.9959 | VlF1: 0.9979 *
8860.5s 653 [GPU 1]   Epoch   2/30 | TrLoss: 0.0189 | VlLoss: 0.0428 | VlAcc: 0.9879 | VlF1: 0.9937 *
8906.3s 654 [GPU 0]   Epoch  10/30 | TrLoss: 0.0014 | VlLoss: 0.0221 | VlAcc: 0.9963 | VlF1: 0.9981 *
8977.0s 655 [GPU 1]   Epoch   3/30 | TrLoss: 0.0124 | VlLoss: 0.1423 | VlAcc: 0.9893 | VlF1: 0.9944 *
8993.7s 656 [GPU 0]   Epoch  11/30 | TrLoss: 0.0013 | VlLoss: 0.0273 | VlAcc: 0.9959 | VlF1: 0.9979
9080.9s 657 [GPU 0]   Epoch  12/30 | TrLoss: 0.0011 | VlLoss: 0.0297 | VlAcc: 0.9957 | VlF1: 0.9978
9093.2s 658 [GPU 1]   Epoch   4/30 | TrLoss: 0.0054 | VlLoss: 0.0624 | VlAcc: 0.9893 | VlF1: 0.9944 *
9168.3s 659 [GPU 0]   Epoch  13/30 | TrLoss: 0.0010 | VlLoss: 0.0202 | VlAcc: 0.9966 | VlF1: 0.9982 *
9209.8s 660 [GPU 1]   Epoch   5/30 | TrLoss: 0.0031 | VlLoss: 0.0819 | VlAcc: 0.9773 | VlF1: 0.9884
9255.7s 661 [GPU 0]   Epoch  14/30 | TrLoss: 0.0010 | VlLoss: 0.0218 | VlAcc: 0.9961 | VlF1: 0.9980
9326.3s 662 [GPU 1]   Epoch   6/30 | TrLoss: 0.0027 | VlLoss: 0.1042 | VlAcc: 0.9893 | VlF1: 0.9944 *
9342.9s 663 [GPU 0]   Epoch  15/30 | TrLoss: 0.0009 | VlLoss: 0.0412 | VlAcc: 0.9939 | VlF1: 0.9968
9429.8s 664 [GPU 0]   Epoch  16/30 | TrLoss: 0.0008 | VlLoss: 0.0265 | VlAcc: 0.9950 | VlF1: 0.9974
9442.4s 665 [GPU 1]   Epoch   7/30 | TrLoss: 0.0020 | VlLoss: 0.0879 | VlAcc: 0.9831 | VlF1: 0.9913
9517.1s 666 [GPU 0]   Epoch  17/30 | TrLoss: 0.0005 | VlLoss: 0.0226 | VlAcc: 0.9961 | VlF1: 0.9980
9558.5s 667 [GPU 1]   Epoch   8/30 | TrLoss: 0.0016 | VlLoss: 0.0653 | VlAcc: 0.9785 | VlF1: 0.9889
9604.1s 668 [GPU 0]   Epoch  18/30 | TrLoss: 0.0004 | VlLoss: 0.0205 | VlAcc: 0.9962 | VlF1: 0.9980
9604.1s 669 [GPU 0]   Early stopping at epoch 18 (patience=5)
9604.1s 670 [GPU 0]   Best Val F1: 0.9982 (18 epochs, 1572.5s)
9604.1s 671 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_CNN-LSTM_seed42.pt
9604.1s 672 [GPU 0] [4/5] Evaluating on test set...
9615.6s 673 [GPU 0]   Test Acc: 0.9721 | Prec: 0.7871 | Rec: 0.9429 | F1: 0.8580 | AUC: 0.9770
9615.6s 674 [GPU 0] [5/5] Measuring efficiency...
9615.8s 675 [GPU 0]   Latency: 0.99ms | Throughput: 33118/s | Memory: 1.56MB
9627.1s 676 [GPU 0]   Attack types analyzed: 3
9627.1s 677 [GPU 0]
9627.1s 678 [GPU 0] [6/18] Running experiment...
9627.1s 679 [GPU 0]
9627.1s 680 [GPU 0] ======================================================================
9627.1s 681 [GPU 0]   Dataset: CICIDS2017 | Model: TCN | Seed: 42
9627.1s 682 [GPU 0] ======================================================================
9627.1s 683 [GPU 0] [Reproducibility] Setting all random seeds to 42
9627.1s 684 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
9627.1s 685 [GPU 0] [1/5] Loading CICIDS2017 dataset...
9627.1s 686 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
9628.3s 687 [GPU 0]   Cached shape: (863214, 77), labels: 863214
9628.3s 688 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
9628.3s 689 [GPU 0] Fitting StandardScaler on Train split...
9629.2s 690 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
9629.2s 691 [GPU 0] [OK] Will generate 863165 windows lazily during training
9629.2s 692 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
9629.3s 693 [GPU 0]   Cached shape: (123316, 77), labels: 123316
9629.3s 694 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
9629.3s 695 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
9629.3s 696 [GPU 0] [OK] Will generate 123267 windows lazily during training
9629.3s 697 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
9629.6s 698 [GPU 0]   Cached shape: (246633, 77), labels: 246633
9629.6s 699 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
9629.6s 700 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
9629.8s 701 [GPU 0] [OK] Will generate 246584 windows lazily during training
9629.8s 702 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
9629.8s 703 [GPU 0] [2/5] Initializing TCN (d_model=128, n_layers=2)...
9629.8s 704 [GPU 0]   Parameters: 423,169
9629.8s 705 [GPU 0] [3/5] Training TCN...
9629.8s 706 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for TCN), grad_clip=1.0
9629.8s 707 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
9674.8s 708 [GPU 1]   Epoch   9/30 | TrLoss: 0.0013 | VlLoss: 0.0768 | VlAcc: 0.9755 | VlF1: 0.9874
9739.6s 709 [GPU 0]   Epoch   1/30 | TrLoss: 0.0327 | VlLoss: 0.0974 | VlAcc: 0.9751 | VlF1: 0.9873 *
9790.9s 710 [GPU 1]   Epoch  10/30 | TrLoss: 0.0012 | VlLoss: 0.0603 | VlAcc: 0.9796 | VlF1: 0.9895
9849.9s 711 [GPU 0]   Epoch   2/30 | TrLoss: 0.0259 | VlLoss: 0.0587 | VlAcc: 0.9786 | VlF1: 0.9890 *
9907.3s 712 [GPU 1]   Epoch  11/30 | TrLoss: 0.0009 | VlLoss: 0.0729 | VlAcc: 0.9792 | VlF1: 0.9893
9907.3s 713 [GPU 1]   Early stopping at epoch 11 (patience=5)
9907.3s 714 [GPU 1]   Best Val F1: 0.9944 (11 epochs, 1279.4s)
9907.3s 715 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_TCN_seed789.pt
9907.3s 716 [GPU 1] [4/5] Evaluating on test set...
9918.4s 717 [GPU 1]   Test Acc: 0.8992 | Prec: 0.4684 | Rec: 0.9504 | F1: 0.6275 | AUC: 0.9683
9918.4s 718 [GPU 1] [5/5] Measuring efficiency...
9918.9s 719 [GPU 1]   Latency: 2.23ms | Throughput: 14846/s | Memory: 1.61MB
9929.8s 720 [GPU 1]   Attack types analyzed: 3
9929.8s 721 [GPU 1]
9929.8s 722 [GPU 1] [7/12] Running experiment...
9929.8s 723 [GPU 1]
9929.8s 724 [GPU 1] ======================================================================
9929.8s 725 [GPU 1]   Dataset: CICIDS2017 | Model: Mamba | Seed: 1024
9929.8s 726 [GPU 1] ======================================================================
9929.8s 727 [GPU 1] [Reproducibility] Setting all random seeds to 1024
9929.8s 728 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
9929.8s 729 [GPU 1] [1/5] Loading CICIDS2017 dataset...
9929.8s 730 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
9930.9s 731 [GPU 1]   Cached shape: (863214, 77), labels: 863214
9930.9s 732 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
9930.9s 733 [GPU 1] Fitting StandardScaler on Train split...
9931.9s 734 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
9931.9s 735 [GPU 1] [OK] Will generate 863165 windows lazily during training
9931.9s 736 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
9932.0s 737 [GPU 1]   Cached shape: (123316, 77), labels: 123316
9932.0s 738 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
9932.0s 739 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
9932.0s 740 [GPU 1] [OK] Will generate 123267 windows lazily during training
9932.0s 741 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
9932.3s 742 [GPU 1]   Cached shape: (246633, 77), labels: 246633
9932.3s 743 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
9932.3s 744 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
9932.4s 745 [GPU 1] [OK] Will generate 246584 windows lazily during training
9932.4s 746 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
9932.4s 747 [GPU 1] [2/5] Initializing Mamba (d_model=128, n_layers=2)...
9932.4s 748 [GPU 1]   Parameters: 387,073
9932.5s 749 [GPU 1] [3/5] Training Mamba...
9932.5s 750 [GPU 1]   LR: 0.000300 (base=0.001 Ã— 0.3 for Mamba), grad_clip=0.5
9932.5s 751 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
9960.0s 752 [GPU 0]   Epoch   3/30 | TrLoss: 0.0116 | VlLoss: 0.1579 | VlAcc: 0.9784 | VlF1: 0.9888
10069.6s 753 [GPU 0]   Epoch   4/30 | TrLoss: 0.0053 | VlLoss: 0.0573 | VlAcc: 0.9806 | VlF1: 0.9900 *
10179.0s 754 [GPU 0]   Epoch   5/30 | TrLoss: 0.0040 | VlLoss: 0.0934 | VlAcc: 0.9776 | VlF1: 0.9885
10288.3s 755 [GPU 0]   Epoch   6/30 | TrLoss: 0.0029 | VlLoss: 0.1239 | VlAcc: 0.9762 | VlF1: 0.9877
10398.2s 756 [GPU 0]   Epoch   7/30 | TrLoss: 0.0026 | VlLoss: 0.1445 | VlAcc: 0.9773 | VlF1: 0.9883
10507.3s 757 [GPU 0]   Epoch   8/30 | TrLoss: 0.0021 | VlLoss: 0.1522 | VlAcc: 0.9766 | VlF1: 0.9879
10615.6s 758 [GPU 1]   Epoch   1/30 | TrLoss: 0.0209 | VlLoss: 0.0357 | VlAcc: 0.9952 | VlF1: 0.9975 *
10616.8s 759 [GPU 0]   Epoch   9/30 | TrLoss: 0.0014 | VlLoss: 0.1563 | VlAcc: 0.9778 | VlF1: 0.9886
10616.8s 760 [GPU 0]   Early stopping at epoch 9 (patience=5)
10616.8s 761 [GPU 0]   Best Val F1: 0.9900 (9 epochs, 986.9s)
10616.8s 762 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_TCN_seed42.pt
10616.8s 763 [GPU 0] [4/5] Evaluating on test set...
10627.4s 764 [GPU 0]   Test Acc: 0.5014 | Prec: 0.1493 | Rec: 0.9748 | F1: 0.2589 | AUC: 0.9688
10627.4s 765 [GPU 0] [5/5] Measuring efficiency...
10627.8s 766 [GPU 0]   Latency: 1.73ms | Throughput: 16754/s | Memory: 1.61MB
10638.4s 767 [GPU 0]   Attack types analyzed: 3
10638.4s 768 [GPU 0]
10638.4s 769 [GPU 0] [7/18] Running experiment...
10638.4s 770 [GPU 0]
10638.4s 771 [GPU 0] ======================================================================
10638.4s 772 [GPU 0]   Dataset: CICIDS2017 | Model: Mamba | Seed: 123
10638.4s 773 [GPU 0] ======================================================================
10638.4s 774 [GPU 0] [Reproducibility] Setting all random seeds to 123
10638.4s 775 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
10638.4s 776 [GPU 0] [1/5] Loading CICIDS2017 dataset...
10638.4s 777 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
10639.5s 778 [GPU 0]   Cached shape: (863214, 77), labels: 863214
10639.5s 779 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
10639.5s 780 [GPU 0] Fitting StandardScaler on Train split...
10640.4s 781 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
10640.4s 782 [GPU 0] [OK] Will generate 863165 windows lazily during training
10640.4s 783 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
10640.5s 784 [GPU 0]   Cached shape: (123316, 77), labels: 123316
10640.5s 785 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
10640.5s 786 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
10640.6s 787 [GPU 0] [OK] Will generate 123267 windows lazily during training
10640.6s 788 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
10640.9s 789 [GPU 0]   Cached shape: (246633, 77), labels: 246633
10640.9s 790 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
10640.9s 791 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
10641.1s 792 [GPU 0] [OK] Will generate 246584 windows lazily during training
10641.1s 793 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
10641.1s 794 [GPU 0] [2/5] Initializing Mamba (d_model=128, n_layers=2)...
10641.1s 795 [GPU 0]   Parameters: 387,073
10641.2s 796 [GPU 0] [3/5] Training Mamba...
10641.2s 797 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Mamba), grad_clip=0.5
10641.2s 798 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
11297.9s 799 [GPU 1]   Epoch   2/30 | TrLoss: 0.0076 | VlLoss: 0.0192 | VlAcc: 0.9962 | VlF1: 0.9980 *
11304.1s 800 [GPU 0]   Epoch   1/30 | TrLoss: 0.0183 | VlLoss: 0.0184 | VlAcc: 0.9972 | VlF1: 0.9986 *
11969.2s 801 [GPU 0]   Epoch   2/30 | TrLoss: 0.0201 | VlLoss: 0.0651 | VlAcc: 0.9884 | VlF1: 0.9940
11981.9s 802 [GPU 1]   Epoch   3/30 | TrLoss: 0.0140 | VlLoss: 0.0263 | VlAcc: 0.9949 | VlF1: 0.9973
12634.8s 803 [GPU 0]   Epoch   3/30 | TrLoss: 0.0407 | VlLoss: 0.0154 | VlAcc: 0.9957 | VlF1: 0.9978
12666.1s 804 [GPU 1]   Epoch   4/30 | TrLoss: 0.0056 | VlLoss: 0.0220 | VlAcc: 0.9946 | VlF1: 0.9972
13297.0s 805 [GPU 0]   Epoch   4/30 | TrLoss: 0.0126 | VlLoss: 0.0404 | VlAcc: 0.9897 | VlF1: 0.9947
13348.2s 806 [GPU 1]   Epoch   5/30 | TrLoss: 0.0053 | VlLoss: 0.0161 | VlAcc: 0.9972 | VlF1: 0.9985 *
13962.9s 807 [GPU 0]   Epoch   5/30 | TrLoss: 0.0042 | VlLoss: 0.0255 | VlAcc: 0.9945 | VlF1: 0.9971
14031.5s 808 [GPU 1]   Epoch   6/30 | TrLoss: 0.0026 | VlLoss: 0.1425 | VlAcc: 0.9524 | VlF1: 0.9747
14634.0s 809 [GPU 0]   Epoch   6/30 | TrLoss: 0.0041 | VlLoss: 0.0219 | VlAcc: 0.9943 | VlF1: 0.9970
14634.0s 810 [GPU 0]   Early stopping at epoch 6 (patience=5)
14634.0s 811 [GPU 0]   Best Val F1: 0.9986 (6 epochs, 3992.8s)
14634.0s 812 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Mamba_seed123.pt
14634.0s 813 [GPU 0] [4/5] Evaluating on test set...
14677.4s 814 [GPU 0]   Test Acc: 0.9815 | Prec: 0.8560 | Rec: 0.9530 | F1: 0.9019 | AUC: 0.9847
14677.4s 815 [GPU 0] [5/5] Measuring efficiency...
14680.6s 816 [GPU 0]   Latency: 15.20ms | Throughput: 2075/s | Memory: 1.48MB
14719.4s 817 [GPU 1]   Epoch   7/30 | TrLoss: 0.0030 | VlLoss: 0.0176 | VlAcc: 0.9961 | VlF1: 0.9980
14723.4s 818 [GPU 0]   Attack types analyzed: 3
14723.4s 819 [GPU 0]
14723.4s 820 [GPU 0] [8/18] Running experiment...
14723.4s 821 [GPU 0]
14723.4s 822 [GPU 0] ======================================================================
14723.4s 823 [GPU 0]   Dataset: CICIDS2017 | Model: LSTM | Seed: 123
14723.4s 824 [GPU 0] ======================================================================
14723.4s 825 [GPU 0] [Reproducibility] Setting all random seeds to 123
14723.4s 826 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
14723.4s 827 [GPU 0] [1/5] Loading CICIDS2017 dataset...
14723.4s 828 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
14724.5s 829 [GPU 0]   Cached shape: (863214, 77), labels: 863214
14724.5s 830 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
14724.5s 831 [GPU 0] Fitting StandardScaler on Train split...
14725.4s 832 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
14725.4s 833 [GPU 0] [OK] Will generate 863165 windows lazily during training
14725.4s 834 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
14725.5s 835 [GPU 0]   Cached shape: (123316, 77), labels: 123316
14725.5s 836 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
14725.5s 837 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
14725.5s 838 [GPU 0] [OK] Will generate 123267 windows lazily during training
14725.5s 839 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
14725.8s 840 [GPU 0]   Cached shape: (246633, 77), labels: 246633
14725.8s 841 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
14725.8s 842 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
14725.9s 843 [GPU 0] [OK] Will generate 246584 windows lazily during training
14725.9s 844 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
14725.9s 845 [GPU 0] [2/5] Initializing LSTM (d_model=128, n_layers=2)...
14725.9s 846 [GPU 0]   Parameters: 423,169
14726.0s 847 [GPU 0] [3/5] Training LSTM...
14726.0s 848 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for LSTM), grad_clip=1.0
14726.0s 849 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
14814.0s 850 [GPU 0]   Epoch   1/30 | TrLoss: 0.0436 | VlLoss: 0.1854 | VlAcc: 0.9642 | VlF1: 0.9818 *
14902.0s 851 [GPU 0]   Epoch   2/30 | TrLoss: 0.0535 | VlLoss: 0.1849 | VlAcc: 0.9642 | VlF1: 0.9818
14990.2s 852 [GPU 0]   Epoch   3/30 | TrLoss: 0.2197 | VlLoss: 0.1586 | VlAcc: 0.9642 | VlF1: 0.9818
15078.0s 853 [GPU 0]   Epoch   4/30 | TrLoss: 0.0528 | VlLoss: 0.1806 | VlAcc: 0.9642 | VlF1: 0.9818
15166.2s 854 [GPU 0]   Epoch   5/30 | TrLoss: 0.0382 | VlLoss: 0.0495 | VlAcc: 0.9876 | VlF1: 0.9936 *
15254.6s 855 [GPU 0]   Epoch   6/30 | TrLoss: 0.0086 | VlLoss: 0.0266 | VlAcc: 0.9946 | VlF1: 0.9972 *
15344.7s 856 [GPU 0]   Epoch   7/30 | TrLoss: 0.0052 | VlLoss: 0.0158 | VlAcc: 0.9970 | VlF1: 0.9985 *
15409.6s 857 [GPU 1]   Epoch   8/30 | TrLoss: 0.0020 | VlLoss: 0.0398 | VlAcc: 0.9935 | VlF1: 0.9966
15436.8s 858 [GPU 0]   Epoch   8/30 | TrLoss: 0.0018 | VlLoss: 0.0179 | VlAcc: 0.9969 | VlF1: 0.9984
15528.0s 859 [GPU 0]   Epoch   9/30 | TrLoss: 0.0017 | VlLoss: 0.0178 | VlAcc: 0.9968 | VlF1: 0.9983
15618.6s 860 [GPU 0]   Epoch  10/30 | TrLoss: 0.0014 | VlLoss: 0.0201 | VlAcc: 0.9967 | VlF1: 0.9983
15708.0s 861 [GPU 0]   Epoch  11/30 | TrLoss: 0.0012 | VlLoss: 0.0216 | VlAcc: 0.9967 | VlF1: 0.9983
15796.7s 862 [GPU 0]   Epoch  12/30 | TrLoss: 0.0010 | VlLoss: 0.0232 | VlAcc: 0.9967 | VlF1: 0.9983
15796.7s 863 [GPU 0]   Early stopping at epoch 12 (patience=5)
15796.7s 864 [GPU 0]   Best Val F1: 0.9985 (12 epochs, 1070.7s)
15796.7s 865 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_LSTM_seed123.pt
15796.7s 866 [GPU 0] [4/5] Evaluating on test set...
15809.5s 867 [GPU 0]   Test Acc: 0.9745 | Prec: 0.7958 | Rec: 0.9612 | F1: 0.8707 | AUC: 0.9872
15809.5s 868 [GPU 0] [5/5] Measuring efficiency...
15809.7s 869 [GPU 0]   Latency: 0.78ms | Throughput: 30704/s | Memory: 1.61MB
15822.5s 870 [GPU 0]   Attack types analyzed: 3
15822.5s 871 [GPU 0]
15822.5s 872 [GPU 0] [9/18] Running experiment...
15822.5s 873 [GPU 0]
15822.5s 874 [GPU 0] ======================================================================
15822.5s 875 [GPU 0]   Dataset: CICIDS2017 | Model: GRU | Seed: 123
15822.5s 876 [GPU 0] ======================================================================
15822.5s 877 [GPU 0] [Reproducibility] Setting all random seeds to 123
15822.5s 878 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
15822.5s 879 [GPU 0] [1/5] Loading CICIDS2017 dataset...
15822.5s 880 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
15823.7s 881 [GPU 0]   Cached shape: (863214, 77), labels: 863214
15823.7s 882 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
15823.7s 883 [GPU 0] Fitting StandardScaler on Train split...
15824.7s 884 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
15824.7s 885 [GPU 0] [OK] Will generate 863165 windows lazily during training
15824.7s 886 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
15824.8s 887 [GPU 0]   Cached shape: (123316, 77), labels: 123316
15824.8s 888 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
15824.8s 889 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
15824.9s 890 [GPU 0] [OK] Will generate 123267 windows lazily during training
15824.9s 891 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
15825.2s 892 [GPU 0]   Cached shape: (246633, 77), labels: 246633
15825.2s 893 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
15825.2s 894 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
15825.3s 895 [GPU 0] [OK] Will generate 246584 windows lazily during training
15825.3s 896 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
15825.3s 897 [GPU 0] [2/5] Initializing GRU (d_model=128, n_layers=2)...
15825.3s 898 [GPU 0]   Parameters: 373,505
15825.4s 899 [GPU 0] [3/5] Training GRU...
15825.4s 900 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for GRU), grad_clip=1.0
15825.4s 901 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
15900.5s 902 [GPU 0]   Epoch   1/30 | TrLoss: 0.0557 | VlLoss: 0.1876 | VlAcc: 0.9642 | VlF1: 0.9818 *
15977.5s 903 [GPU 0]   Epoch   2/30 | TrLoss: 0.0564 | VlLoss: 0.1859 | VlAcc: 0.9642 | VlF1: 0.9818
16056.6s 904 [GPU 0]   Epoch   3/30 | TrLoss: 0.0489 | VlLoss: 0.1824 | VlAcc: 0.9642 | VlF1: 0.9818
16115.8s 905 [GPU 1]   Epoch   9/30 | TrLoss: 0.0016 | VlLoss: 0.1337 | VlAcc: 0.9901 | VlF1: 0.9948
16137.2s 906 [GPU 0]   Epoch   4/30 | TrLoss: 0.0356 | VlLoss: 0.1808 | VlAcc: 0.9642 | VlF1: 0.9818
16216.1s 907 [GPU 0]   Epoch   5/30 | TrLoss: 0.0449 | VlLoss: 0.1830 | VlAcc: 0.9642 | VlF1: 0.9818
16294.1s 908 [GPU 0]   Epoch   6/30 | TrLoss: 0.0330 | VlLoss: 0.1831 | VlAcc: 0.9642 | VlF1: 0.9818
16294.1s 909 [GPU 0]   Early stopping at epoch 6 (patience=5)
16294.1s 910 [GPU 0]   Best Val F1: 0.9818 (6 epochs, 468.7s)
16294.1s 911 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_GRU_seed123.pt
16294.1s 912 [GPU 0] [4/5] Evaluating on test set...
16306.5s 913 [GPU 0]   Test Acc: 0.0896 | Prec: 0.0894 | Rec: 1.0000 | F1: 0.1641 | AUC: 0.7463
16306.5s 914 [GPU 0] [5/5] Measuring efficiency...
16306.7s 915 [GPU 0]   Latency: 0.58ms | Throughput: 43908/s | Memory: 1.42MB
16317.9s 916 [GPU 0]   Attack types analyzed: 3
16317.9s 917 [GPU 0]
16317.9s 918 [GPU 0] [10/18] Running experiment...
16317.9s 919 [GPU 0]
16317.9s 920 [GPU 0] ======================================================================
16317.9s 921 [GPU 0]   Dataset: CICIDS2017 | Model: Transformer | Seed: 123
16317.9s 922 [GPU 0] ======================================================================
16317.9s 923 [GPU 0] [Reproducibility] Setting all random seeds to 123
16317.9s 924 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
16317.9s 925 [GPU 0] [1/5] Loading CICIDS2017 dataset...
16317.9s 926 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
16319.1s 927 [GPU 0]   Cached shape: (863214, 77), labels: 863214
16319.1s 928 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
16319.1s 929 [GPU 0] Fitting StandardScaler on Train split...
16320.1s 930 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
16320.1s 931 [GPU 0] [OK] Will generate 863165 windows lazily during training
16320.1s 932 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
16320.2s 933 [GPU 0]   Cached shape: (123316, 77), labels: 123316
16320.2s 934 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
16320.2s 935 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
16320.3s 936 [GPU 0] [OK] Will generate 123267 windows lazily during training
16320.3s 937 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
16320.6s 938 [GPU 0]   Cached shape: (246633, 77), labels: 246633
16320.6s 939 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
16320.6s 940 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
16320.7s 941 [GPU 0] [OK] Will generate 246584 windows lazily during training
16320.7s 942 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
16320.7s 943 [GPU 0] [2/5] Initializing Transformer (d_model=128, n_layers=2)...
16320.7s 944 [GPU 0]   Parameters: 406,913
16320.8s 945 [GPU 0] [3/5] Training Transformer...
16320.8s 946 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Transformer), grad_clip=0.5
16320.8s 947 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
16436.0s 948 [GPU 0]   Epoch   1/30 | TrLoss: 0.0956 | VlLoss: 0.2134 | VlAcc: 0.9664 | VlF1: 0.9829 *
16551.0s 949 [GPU 0]   Epoch   2/30 | TrLoss: 0.0545 | VlLoss: 0.1858 | VlAcc: 0.9642 | VlF1: 0.9818
16664.8s 950 [GPU 0]   Epoch   3/30 | TrLoss: 0.0445 | VlLoss: 0.1894 | VlAcc: 0.9642 | VlF1: 0.9818
16779.4s 951 [GPU 0]   Epoch   4/30 | TrLoss: 0.0487 | VlLoss: 0.1897 | VlAcc: 0.9642 | VlF1: 0.9818
16894.6s 952 [GPU 0]   Epoch   5/30 | TrLoss: 0.0639 | VlLoss: 0.1963 | VlAcc: 0.9643 | VlF1: 0.9818
16923.1s 953 [GPU 1]   Epoch  10/30 | TrLoss: 0.0012 | VlLoss: 0.0416 | VlAcc: 0.9936 | VlF1: 0.9967
16923.1s 954 [GPU 1]   Early stopping at epoch 10 (patience=5)
16923.1s 955 [GPU 1]   Best Val F1: 0.9985 (10 epochs, 6990.6s)
16923.1s 956 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Mamba_seed1024.pt
16923.1s 957 [GPU 1] [4/5] Evaluating on test set...
16972.2s 958 [GPU 1]   Test Acc: 0.9912 | Prec: 0.9848 | Rec: 0.9151 | F1: 0.9487 | AUC: 0.9808
16972.2s 959 [GPU 1] [5/5] Measuring efficiency...
16976.6s 960 [GPU 1]   Latency: 21.29ms | Throughput: 1603/s | Memory: 1.48MB
17005.0s 961 [GPU 0]   Epoch   6/30 | TrLoss: 0.0629 | VlLoss: 0.2128 | VlAcc: 0.9642 | VlF1: 0.9818
17005.0s 962 [GPU 0]   Early stopping at epoch 6 (patience=5)
17005.0s 963 [GPU 0]   Best Val F1: 0.9829 (6 epochs, 684.1s)
17005.0s 964 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Transformer_seed123.pt
17005.0s 965 [GPU 0] [4/5] Evaluating on test set...
17017.4s 966 [GPU 0]   Test Acc: 0.0898 | Prec: 0.0894 | Rec: 1.0000 | F1: 0.1641 | AUC: 0.7427
17017.4s 967 [GPU 0] [5/5] Measuring efficiency...
17017.7s 968 [GPU 0]   Latency: 1.44ms | Throughput: 23843/s | Memory: 1.55MB
17025.5s 969 [GPU 1]   Attack types analyzed: 3
17025.5s 970 [GPU 1]
17025.5s 971 [GPU 1] [8/12] Running experiment...
17025.5s 972 [GPU 1]
17025.5s 973 [GPU 1] ======================================================================
17025.5s 974 [GPU 1]   Dataset: CICIDS2017 | Model: LSTM | Seed: 1024
17025.5s 975 [GPU 1] ======================================================================
17025.5s 976 [GPU 1] [Reproducibility] Setting all random seeds to 1024
17025.5s 977 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
17025.5s 978 [GPU 1] [1/5] Loading CICIDS2017 dataset...
17025.5s 979 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
17026.8s 980 [GPU 1]   Cached shape: (863214, 77), labels: 863214
17026.8s 981 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
17026.8s 982 [GPU 1] Fitting StandardScaler on Train split...
17027.8s 983 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
17027.8s 984 [GPU 1] [OK] Will generate 863165 windows lazily during training
17027.9s 985 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
17027.9s 986 [GPU 1]   Cached shape: (123316, 77), labels: 123316
17027.9s 987 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
17027.9s 988 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
17028.0s 989 [GPU 1] [OK] Will generate 123267 windows lazily during training
17028.0s 990 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
17028.4s 991 [GPU 1]   Cached shape: (246633, 77), labels: 246633
17028.4s 992 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
17028.4s 993 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
17028.5s 994 [GPU 1] [OK] Will generate 246584 windows lazily during training
17028.5s 995 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
17028.5s 996 [GPU 1] [2/5] Initializing LSTM (d_model=128, n_layers=2)...
17028.5s 997 [GPU 1]   Parameters: 423,169
17028.6s 998 [GPU 1] [3/5] Training LSTM...
17028.6s 999 [GPU 1]   LR: 0.001000 (base=0.001 Ã— 1.0 for LSTM), grad_clip=1.0
17028.6s 1000 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
17029.9s 1001 [GPU 0]   Attack types analyzed: 3
17029.9s 1002 [GPU 0]
17029.9s 1003 [GPU 0] [11/18] Running experiment...
17029.9s 1004 [GPU 0]
17029.9s 1005 [GPU 0] ======================================================================
17029.9s 1006 [GPU 0]   Dataset: CICIDS2017 | Model: CNN-LSTM | Seed: 123
17029.9s 1007 [GPU 0] ======================================================================
17029.9s 1008 [GPU 0] [Reproducibility] Setting all random seeds to 123
17029.9s 1009 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
17029.9s 1010 [GPU 0] [1/5] Loading CICIDS2017 dataset...
17029.9s 1011 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
17031.3s 1012 [GPU 0]   Cached shape: (863214, 77), labels: 863214
17031.3s 1013 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
17031.3s 1014 [GPU 0] Fitting StandardScaler on Train split...
17032.4s 1015 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
17032.4s 1016 [GPU 0] [OK] Will generate 863165 windows lazily during training
17032.4s 1017 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
17032.4s 1018 [GPU 0]   Cached shape: (123316, 77), labels: 123316
17032.4s 1019 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
17032.4s 1020 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
17032.5s 1021 [GPU 0] [OK] Will generate 123267 windows lazily during training
17032.5s 1022 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
17032.8s 1023 [GPU 0]   Cached shape: (246633, 77), labels: 246633
17032.8s 1024 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
17032.8s 1025 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
17032.9s 1026 [GPU 0] [OK] Will generate 246584 windows lazily during training
17032.9s 1027 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
17032.9s 1028 [GPU 0] [2/5] Initializing CNN-LSTM (d_model=128, n_layers=2)...
17033.0s 1029 [GPU 0]   Parameters: 409,601
17033.0s 1030 [GPU 0] [3/5] Training CNN-LSTM...
17033.0s 1031 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for CNN-LSTM), grad_clip=0.5
17033.0s 1032 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
17126.5s 1033 [GPU 0]   Epoch   1/30 | TrLoss: 0.0639 | VlLoss: 0.1066 | VlAcc: 0.9750 | VlF1: 0.9872 *
17127.5s 1034 [GPU 1]   Epoch   1/30 | TrLoss: 0.0434 | VlLoss: 0.1844 | VlAcc: 0.9642 | VlF1: 0.9818 *
17218.8s 1035 [GPU 0]   Epoch   2/30 | TrLoss: 0.0734 | VlLoss: 0.1034 | VlAcc: 0.9832 | VlF1: 0.9912 *
17225.4s 1036 [GPU 1]   Epoch   2/30 | TrLoss: 0.0383 | VlLoss: 0.1831 | VlAcc: 0.9642 | VlF1: 0.9818
17310.9s 1037 [GPU 0]   Epoch   3/30 | TrLoss: 0.0172 | VlLoss: 0.3925 | VlAcc: 0.8405 | VlF1: 0.9104
17322.6s 1038 [GPU 1]   Epoch   3/30 | TrLoss: 0.0319 | VlLoss: 0.1785 | VlAcc: 0.9642 | VlF1: 0.9818
17402.1s 1039 [GPU 0]   Epoch   4/30 | TrLoss: 0.0136 | VlLoss: 0.5869 | VlAcc: 0.4864 | VlF1: 0.6379
17418.7s 1040 [GPU 1]   Epoch   4/30 | TrLoss: 0.0318 | VlLoss: 0.1746 | VlAcc: 0.9642 | VlF1: 0.9818
17491.3s 1041 [GPU 0]   Epoch   5/30 | TrLoss: 0.0039 | VlLoss: 0.0923 | VlAcc: 0.9921 | VlF1: 0.9959 *
17513.0s 1042 [GPU 1]   Epoch   5/30 | TrLoss: 0.0315 | VlLoss: 0.1862 | VlAcc: 0.9642 | VlF1: 0.9818
17579.7s 1043 [GPU 0]   Epoch   6/30 | TrLoss: 0.0028 | VlLoss: 0.0626 | VlAcc: 0.9937 | VlF1: 0.9967 *
17607.1s 1044 [GPU 1]   Epoch   6/30 | TrLoss: 0.0353 | VlLoss: 0.1848 | VlAcc: 0.9642 | VlF1: 0.9818
17607.1s 1045 [GPU 1]   Early stopping at epoch 6 (patience=5)
17607.1s 1046 [GPU 1]   Best Val F1: 0.9818 (6 epochs, 578.5s)
17607.1s 1047 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_LSTM_seed1024.pt
17607.1s 1048 [GPU 1] [4/5] Evaluating on test set...
17620.4s 1049 [GPU 1]   Test Acc: 0.0893 | Prec: 0.0893 | Rec: 1.0000 | F1: 0.1640 | AUC: 0.4414
17620.4s 1050 [GPU 1] [5/5] Measuring efficiency...
17620.6s 1051 [GPU 1]   Latency: 0.83ms | Throughput: 28119/s | Memory: 1.61MB
17633.8s 1052 [GPU 1]   Attack types analyzed: 3
17633.8s 1053 [GPU 1]
17633.8s 1054 [GPU 1] [9/12] Running experiment...
17633.8s 1055 [GPU 1]
17633.8s 1056 [GPU 1] ======================================================================
17633.8s 1057 [GPU 1]   Dataset: CICIDS2017 | Model: GRU | Seed: 1024
17633.8s 1058 [GPU 1] ======================================================================
17633.8s 1059 [GPU 1] [Reproducibility] Setting all random seeds to 1024
17633.8s 1060 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
17633.8s 1061 [GPU 1] [1/5] Loading CICIDS2017 dataset...
17633.8s 1062 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
17635.0s 1063 [GPU 1]   Cached shape: (863214, 77), labels: 863214
17635.0s 1064 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
17635.0s 1065 [GPU 1] Fitting StandardScaler on Train split...
17635.9s 1066 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
17635.9s 1067 [GPU 1] [OK] Will generate 863165 windows lazily during training
17635.9s 1068 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
17636.1s 1069 [GPU 1]   Cached shape: (123316, 77), labels: 123316
17636.1s 1070 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
17636.1s 1071 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
17636.1s 1072 [GPU 1] [OK] Will generate 123267 windows lazily during training
17636.1s 1073 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
17636.4s 1074 [GPU 1]   Cached shape: (246633, 77), labels: 246633
17636.4s 1075 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
17636.4s 1076 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
17636.5s 1077 [GPU 1] [OK] Will generate 246584 windows lazily during training
17636.5s 1078 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
17636.5s 1079 [GPU 1] [2/5] Initializing GRU (d_model=128, n_layers=2)...
17636.5s 1080 [GPU 1]   Parameters: 373,505
17636.6s 1081 [GPU 1] [3/5] Training GRU...
17636.6s 1082 [GPU 1]   LR: 0.001000 (base=0.001 Ã— 1.0 for GRU), grad_clip=1.0
17636.6s 1083 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
17667.8s 1084 [GPU 0]   Epoch   7/30 | TrLoss: 0.0021 | VlLoss: 0.0897 | VlAcc: 0.9960 | VlF1: 0.9979 *
17714.1s 1085 [GPU 1]   Epoch   1/30 | TrLoss: 0.0850 | VlLoss: 0.1794 | VlAcc: 0.9642 | VlF1: 0.9818 *
17756.2s 1086 [GPU 0]   Epoch   8/30 | TrLoss: 0.0019 | VlLoss: 0.0265 | VlAcc: 0.9956 | VlF1: 0.9977
17792.1s 1087 [GPU 1]   Epoch   2/30 | TrLoss: 0.2703 | VlLoss: 0.1858 | VlAcc: 0.9642 | VlF1: 0.9818
17844.5s 1088 [GPU 0]   Epoch   9/30 | TrLoss: 0.0018 | VlLoss: 0.0276 | VlAcc: 0.9952 | VlF1: 0.9975
17868.5s 1089 [GPU 1]   Epoch   3/30 | TrLoss: 0.1024 | VlLoss: 0.1858 | VlAcc: 0.9642 | VlF1: 0.9818
17932.4s 1090 [GPU 0]   Epoch  10/30 | TrLoss: 0.0015 | VlLoss: 0.0302 | VlAcc: 0.9931 | VlF1: 0.9964
17945.9s 1091 [GPU 1]   Epoch   4/30 | TrLoss: 0.0422 | VlLoss: 0.1718 | VlAcc: 0.9642 | VlF1: 0.9818
18020.3s 1092 [GPU 0]   Epoch  11/30 | TrLoss: 0.0012 | VlLoss: 0.0322 | VlAcc: 0.9923 | VlF1: 0.9960
18023.5s 1093 [GPU 1]   Epoch   5/30 | TrLoss: 0.0363 | VlLoss: 0.1415 | VlAcc: 0.9642 | VlF1: 0.9818
18100.7s 1094 [GPU 1]   Epoch   6/30 | TrLoss: 0.0266 | VlLoss: 0.0290 | VlAcc: 0.9949 | VlF1: 0.9974 *
18108.3s 1095 [GPU 0]   Epoch  12/30 | TrLoss: 0.0009 | VlLoss: 0.0460 | VlAcc: 0.9934 | VlF1: 0.9966
18108.3s 1096 [GPU 0]   Early stopping at epoch 12 (patience=5)
18108.3s 1097 [GPU 0]   Best Val F1: 0.9979 (12 epochs, 1075.3s)
18108.3s 1098 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_CNN-LSTM_seed123.pt
18108.3s 1099 [GPU 0] [4/5] Evaluating on test set...
18119.9s 1100 [GPU 0]   Test Acc: 0.9570 | Prec: 0.6860 | Rec: 0.9563 | F1: 0.7989 | AUC: 0.9720
18119.9s 1101 [GPU 0] [5/5] Measuring efficiency...
18120.1s 1102 [GPU 0]   Latency: 1.02ms | Throughput: 34281/s | Memory: 1.56MB
18131.9s 1103 [GPU 0]   Attack types analyzed: 3
18131.9s 1104 [GPU 0]
18131.9s 1105 [GPU 0] [12/18] Running experiment...
18131.9s 1106 [GPU 0]
18131.9s 1107 [GPU 0] ======================================================================
18131.9s 1108 [GPU 0]   Dataset: CICIDS2017 | Model: TCN | Seed: 123
18131.9s 1109 [GPU 0] ======================================================================
18131.9s 1110 [GPU 0] [Reproducibility] Setting all random seeds to 123
18131.9s 1111 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
18131.9s 1112 [GPU 0] [1/5] Loading CICIDS2017 dataset...
18131.9s 1113 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
18133.0s 1114 [GPU 0]   Cached shape: (863214, 77), labels: 863214
18133.0s 1115 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
18133.0s 1116 [GPU 0] Fitting StandardScaler on Train split...
18134.0s 1117 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
18134.0s 1118 [GPU 0] [OK] Will generate 863165 windows lazily during training
18134.0s 1119 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
18134.1s 1120 [GPU 0]   Cached shape: (123316, 77), labels: 123316
18134.1s 1121 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
18134.1s 1122 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
18134.2s 1123 [GPU 0] [OK] Will generate 123267 windows lazily during training
18134.2s 1124 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
18134.5s 1125 [GPU 0]   Cached shape: (246633, 77), labels: 246633
18134.5s 1126 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
18134.5s 1127 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
18134.6s 1128 [GPU 0] [OK] Will generate 246584 windows lazily during training
18134.6s 1129 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
18134.6s 1130 [GPU 0] [2/5] Initializing TCN (d_model=128, n_layers=2)...
18134.6s 1131 [GPU 0]   Parameters: 423,169
18134.7s 1132 [GPU 0] [3/5] Training TCN...
18134.7s 1133 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for TCN), grad_clip=1.0
18134.7s 1134 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
18178.0s 1135 [GPU 1]   Epoch   7/30 | TrLoss: 0.0053 | VlLoss: 0.0130 | VlAcc: 0.9967 | VlF1: 0.9983 *
18245.6s 1136 [GPU 0]   Epoch   1/30 | TrLoss: 0.0324 | VlLoss: 2.5748 | VlAcc: 0.3489 | VlF1: 0.4917 *
18254.6s 1137 [GPU 1]   Epoch   8/30 | TrLoss: 0.0022 | VlLoss: 0.0122 | VlAcc: 0.9973 | VlF1: 0.9986 *
18331.3s 1138 [GPU 1]   Epoch   9/30 | TrLoss: 0.0017 | VlLoss: 0.0105 | VlAcc: 0.9974 | VlF1: 0.9987 *
18356.3s 1139 [GPU 0]   Epoch   2/30 | TrLoss: 0.0105 | VlLoss: 0.0492 | VlAcc: 0.9842 | VlF1: 0.9918 *
18407.3s 1140 [GPU 1]   Epoch  10/30 | TrLoss: 0.0015 | VlLoss: 0.0112 | VlAcc: 0.9972 | VlF1: 0.9986
18466.7s 1141 [GPU 0]   Epoch   3/30 | TrLoss: 0.0065 | VlLoss: 0.0433 | VlAcc: 0.9898 | VlF1: 0.9947 *
18483.7s 1142 [GPU 1]   Epoch  11/30 | TrLoss: 0.0013 | VlLoss: 0.0128 | VlAcc: 0.9970 | VlF1: 0.9984
18559.8s 1143 [GPU 1]   Epoch  12/30 | TrLoss: 0.0011 | VlLoss: 0.0225 | VlAcc: 0.9926 | VlF1: 0.9962
18576.6s 1144 [GPU 0]   Epoch   4/30 | TrLoss: 0.0105 | VlLoss: 0.0404 | VlAcc: 0.9887 | VlF1: 0.9941
18635.7s 1145 [GPU 1]   Epoch  13/30 | TrLoss: 0.0008 | VlLoss: 0.0183 | VlAcc: 0.9956 | VlF1: 0.9977
18686.7s 1146 [GPU 0]   Epoch   5/30 | TrLoss: 0.0039 | VlLoss: 0.0369 | VlAcc: 0.9885 | VlF1: 0.9941
18711.7s 1147 [GPU 1]   Epoch  14/30 | TrLoss: 0.0007 | VlLoss: 0.0245 | VlAcc: 0.9925 | VlF1: 0.9961
18711.7s 1148 [GPU 1]   Early stopping at epoch 14 (patience=5)
18711.7s 1149 [GPU 1]   Best Val F1: 0.9987 (14 epochs, 1075.1s)
18711.7s 1150 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_GRU_seed1024.pt
18711.7s 1151 [GPU 1] [4/5] Evaluating on test set...
18722.7s 1152 [GPU 1]   Test Acc: 0.9915 | Prec: 0.9894 | Rec: 0.9152 | F1: 0.9508 | AUC: 0.9928
18722.7s 1153 [GPU 1] [5/5] Measuring efficiency...
18722.8s 1154 [GPU 1]   Latency: 0.61ms | Throughput: 39153/s | Memory: 1.42MB
18733.8s 1155 [GPU 1]   Attack types analyzed: 3
18733.8s 1156 [GPU 1]
18733.8s 1157 [GPU 1] [10/12] Running experiment...
18733.8s 1158 [GPU 1]
18733.8s 1159 [GPU 1] ======================================================================
18733.8s 1160 [GPU 1]   Dataset: CICIDS2017 | Model: Transformer | Seed: 1024
18733.8s 1161 [GPU 1] ======================================================================
18733.8s 1162 [GPU 1] [Reproducibility] Setting all random seeds to 1024
18733.8s 1163 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
18733.8s 1164 [GPU 1] [1/5] Loading CICIDS2017 dataset...
18733.8s 1165 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
18734.9s 1166 [GPU 1]   Cached shape: (863214, 77), labels: 863214
18734.9s 1167 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
18734.9s 1168 [GPU 1] Fitting StandardScaler on Train split...
18735.9s 1169 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
18735.9s 1170 [GPU 1] [OK] Will generate 863165 windows lazily during training
18735.9s 1171 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
18736.0s 1172 [GPU 1]   Cached shape: (123316, 77), labels: 123316
18736.0s 1173 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
18736.0s 1174 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
18736.0s 1175 [GPU 1] [OK] Will generate 123267 windows lazily during training
18736.0s 1176 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
18736.3s 1177 [GPU 1]   Cached shape: (246633, 77), labels: 246633
18736.3s 1178 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
18736.3s 1179 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
18736.5s 1180 [GPU 1] [OK] Will generate 246584 windows lazily during training
18736.5s 1181 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
18736.5s 1182 [GPU 1] [2/5] Initializing Transformer (d_model=128, n_layers=2)...
18736.5s 1183 [GPU 1]   Parameters: 406,913
18736.6s 1184 [GPU 1] [3/5] Training Transformer...
18736.6s 1185 [GPU 1]   LR: 0.000300 (base=0.001 Ã— 0.3 for Transformer), grad_clip=0.5
18736.6s 1186 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
18797.6s 1187 [GPU 0]   Epoch   6/30 | TrLoss: 0.0026 | VlLoss: 0.0345 | VlAcc: 0.9889 | VlF1: 0.9943
18847.8s 1188 [GPU 1]   Epoch   1/30 | TrLoss: 0.1031 | VlLoss: 0.1247 | VlAcc: 0.9666 | VlF1: 0.9829 *
18910.1s 1189 [GPU 0]   Epoch   7/30 | TrLoss: 0.0021 | VlLoss: 0.0428 | VlAcc: 0.9858 | VlF1: 0.9927
18959.1s 1190 [GPU 1]   Epoch   2/30 | TrLoss: 0.0376 | VlLoss: 0.1602 | VlAcc: 0.9603 | VlF1: 0.9797
19022.9s 1191 [GPU 0]   Epoch   8/30 | TrLoss: 0.0016 | VlLoss: 0.0502 | VlAcc: 0.9852 | VlF1: 0.9924
19022.9s 1192 [GPU 0]   Early stopping at epoch 8 (patience=5)
19022.9s 1193 [GPU 0]   Best Val F1: 0.9947 (8 epochs, 888.2s)
19022.9s 1194 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_TCN_seed123.pt
19022.9s 1195 [GPU 0] [4/5] Evaluating on test set...
19034.6s 1196 [GPU 0]   Test Acc: 0.8865 | Prec: 0.4381 | Rec: 0.9556 | F1: 0.6007 | AUC: 0.9692
19034.6s 1197 [GPU 0] [5/5] Measuring efficiency...
19035.0s 1198 [GPU 0]   Latency: 1.83ms | Throughput: 15605/s | Memory: 1.61MB
19046.8s 1199 [GPU 0]   Attack types analyzed: 3
19046.8s 1200 [GPU 0]
19046.8s 1201 [GPU 0] [13/18] Running experiment...
19046.8s 1202 [GPU 0]
19046.8s 1203 [GPU 0] ======================================================================
19046.8s 1204 [GPU 0]   Dataset: CICIDS2017 | Model: Mamba | Seed: 456
19046.8s 1205 [GPU 0] ======================================================================
19046.8s 1206 [GPU 0] [Reproducibility] Setting all random seeds to 456
19046.8s 1207 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
19046.8s 1208 [GPU 0] [1/5] Loading CICIDS2017 dataset...
19046.8s 1209 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
19048.0s 1210 [GPU 0]   Cached shape: (863214, 77), labels: 863214
19048.0s 1211 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
19048.0s 1212 [GPU 0] Fitting StandardScaler on Train split...
19048.9s 1213 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
19048.9s 1214 [GPU 0] [OK] Will generate 863165 windows lazily during training
19048.9s 1215 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
19049.0s 1216 [GPU 0]   Cached shape: (123316, 77), labels: 123316
19049.0s 1217 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
19049.0s 1218 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
19049.1s 1219 [GPU 0] [OK] Will generate 123267 windows lazily during training
19049.1s 1220 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
19049.4s 1221 [GPU 0]   Cached shape: (246633, 77), labels: 246633
19049.4s 1222 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
19049.4s 1223 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
19049.5s 1224 [GPU 0] [OK] Will generate 246584 windows lazily during training
19049.5s 1225 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
19049.5s 1226 [GPU 0] [2/5] Initializing Mamba (d_model=128, n_layers=2)...
19049.5s 1227 [GPU 0]   Parameters: 387,073
19049.6s 1228 [GPU 0] [3/5] Training Mamba...
19049.6s 1229 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Mamba), grad_clip=0.5
19049.6s 1230 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
19069.9s 1231 [GPU 1]   Epoch   3/30 | TrLoss: 0.0463 | VlLoss: 0.1887 | VlAcc: 0.9642 | VlF1: 0.9818
19182.5s 1232 [GPU 1]   Epoch   4/30 | TrLoss: 0.0571 | VlLoss: 0.1897 | VlAcc: 0.9642 | VlF1: 0.9818
19294.9s 1233 [GPU 1]   Epoch   5/30 | TrLoss: 0.0722 | VlLoss: 0.1764 | VlAcc: 0.9635 | VlF1: 0.9814
19407.0s 1234 [GPU 1]   Epoch   6/30 | TrLoss: 0.0364 | VlLoss: 0.1873 | VlAcc: 0.9568 | VlF1: 0.9779
19407.0s 1235 [GPU 1]   Early stopping at epoch 6 (patience=5)
19407.0s 1236 [GPU 1]   Best Val F1: 0.9829 (6 epochs, 670.4s)
19407.0s 1237 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Transformer_seed1024.pt
19407.0s 1238 [GPU 1] [4/5] Evaluating on test set...
19420.5s 1239 [GPU 1]   Test Acc: 0.1237 | Prec: 0.0918 | Rec: 0.9910 | F1: 0.1681 | AUC: 0.8941
19420.5s 1240 [GPU 1] [5/5] Measuring efficiency...
19420.8s 1241 [GPU 1]   Latency: 1.33ms | Throughput: 25246/s | Memory: 1.55MB
19434.3s 1242 [GPU 1]   Attack types analyzed: 3
19434.3s 1243 [GPU 1]
19434.3s 1244 [GPU 1] [11/12] Running experiment...
19434.3s 1245 [GPU 1]
19434.3s 1246 [GPU 1] ======================================================================
19434.3s 1247 [GPU 1]   Dataset: CICIDS2017 | Model: CNN-LSTM | Seed: 1024
19434.3s 1248 [GPU 1] ======================================================================
19434.3s 1249 [GPU 1] [Reproducibility] Setting all random seeds to 1024
19434.3s 1250 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
19434.3s 1251 [GPU 1] [1/5] Loading CICIDS2017 dataset...
19434.3s 1252 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
19435.5s 1253 [GPU 1]   Cached shape: (863214, 77), labels: 863214
19435.5s 1254 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
19435.5s 1255 [GPU 1] Fitting StandardScaler on Train split...
19436.4s 1256 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
19436.4s 1257 [GPU 1] [OK] Will generate 863165 windows lazily during training
19436.4s 1258 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
19436.5s 1259 [GPU 1]   Cached shape: (123316, 77), labels: 123316
19436.5s 1260 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
19436.5s 1261 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
19436.6s 1262 [GPU 1] [OK] Will generate 123267 windows lazily during training
19436.6s 1263 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
19436.9s 1264 [GPU 1]   Cached shape: (246633, 77), labels: 246633
19436.9s 1265 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
19436.9s 1266 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
19437.0s 1267 [GPU 1] [OK] Will generate 246584 windows lazily during training
19437.0s 1268 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
19437.0s 1269 [GPU 1] [2/5] Initializing CNN-LSTM (d_model=128, n_layers=2)...
19437.0s 1270 [GPU 1]   Parameters: 409,601
19437.1s 1271 [GPU 1] [3/5] Training CNN-LSTM...
19437.1s 1272 [GPU 1]   LR: 0.000500 (base=0.001 Ã— 0.5 for CNN-LSTM), grad_clip=0.5
19437.1s 1273 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
19529.5s 1274 [GPU 1]   Epoch   1/30 | TrLoss: 0.0667 | VlLoss: 0.1854 | VlAcc: 0.9885 | VlF1: 0.9940 *
19622.4s 1275 [GPU 1]   Epoch   2/30 | TrLoss: 0.0180 | VlLoss: 0.0498 | VlAcc: 0.9902 | VlF1: 0.9949 *
19715.2s 1276 [GPU 1]   Epoch   3/30 | TrLoss: 0.0096 | VlLoss: 0.1624 | VlAcc: 0.9925 | VlF1: 0.9961 *
19762.9s 1277 [GPU 0]   Epoch   1/30 | TrLoss: 0.0335 | VlLoss: 0.0433 | VlAcc: 0.9932 | VlF1: 0.9965 *
19807.7s 1278 [GPU 1]   Epoch   4/30 | TrLoss: 0.0130 | VlLoss: 0.3810 | VlAcc: 0.7948 | VlF1: 0.8812
19900.7s 1279 [GPU 1]   Epoch   5/30 | TrLoss: 0.0079 | VlLoss: 0.2022 | VlAcc: 0.9904 | VlF1: 0.9950
19994.0s 1280 [GPU 1]   Epoch   6/30 | TrLoss: 0.0033 | VlLoss: 0.0328 | VlAcc: 0.9944 | VlF1: 0.9971 *
20087.2s 1281 [GPU 1]   Epoch   7/30 | TrLoss: 0.0023 | VlLoss: 0.0327 | VlAcc: 0.9939 | VlF1: 0.9969
20180.9s 1282 [GPU 1]   Epoch   8/30 | TrLoss: 0.0020 | VlLoss: 0.0253 | VlAcc: 0.9963 | VlF1: 0.9981 *
20274.0s 1283 [GPU 1]   Epoch   9/30 | TrLoss: 0.0017 | VlLoss: 0.0233 | VlAcc: 0.9965 | VlF1: 0.9982 *
20367.2s 1284 [GPU 1]   Epoch  10/30 | TrLoss: 0.0014 | VlLoss: 0.0285 | VlAcc: 0.9956 | VlF1: 0.9977
20431.8s 1285 [GPU 0]   Epoch   2/30 | TrLoss: 0.0114 | VlLoss: 0.0260 | VlAcc: 0.9955 | VlF1: 0.9976 *
20459.8s 1286 [GPU 1]   Epoch  11/30 | TrLoss: 0.0014 | VlLoss: 0.0226 | VlAcc: 0.9967 | VlF1: 0.9983 *
20552.4s 1287 [GPU 1]   Epoch  12/30 | TrLoss: 0.0012 | VlLoss: 0.0374 | VlAcc: 0.9949 | VlF1: 0.9973
20645.6s 1288 [GPU 1]   Epoch  13/30 | TrLoss: 0.0010 | VlLoss: 0.0424 | VlAcc: 0.9951 | VlF1: 0.9975
20739.2s 1289 [GPU 1]   Epoch  14/30 | TrLoss: 0.0009 | VlLoss: 0.0334 | VlAcc: 0.9938 | VlF1: 0.9968
20831.8s 1290 [GPU 1]   Epoch  15/30 | TrLoss: 0.0007 | VlLoss: 0.0417 | VlAcc: 0.9907 | VlF1: 0.9952
20924.2s 1291 [GPU 1]   Epoch  16/30 | TrLoss: 0.0006 | VlLoss: 0.0388 | VlAcc: 0.9925 | VlF1: 0.9961
20924.2s 1292 [GPU 1]   Early stopping at epoch 16 (patience=5)
20924.2s 1293 [GPU 1]   Best Val F1: 0.9983 (16 epochs, 1487.0s)
20924.2s 1294 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_CNN-LSTM_seed1024.pt
20924.2s 1295 [GPU 1] [4/5] Evaluating on test set...
20935.6s 1296 [GPU 1]   Test Acc: 0.9755 | Prec: 0.8203 | Rec: 0.9294 | F1: 0.8714 | AUC: 0.9756
20935.6s 1297 [GPU 1] [5/5] Measuring efficiency...
20935.8s 1298 [GPU 1]   Latency: 0.86ms | Throughput: 31569/s | Memory: 1.56MB
20947.2s 1299 [GPU 1]   Attack types analyzed: 3
20947.2s 1300 [GPU 1]
20947.2s 1301 [GPU 1] [12/12] Running experiment...
20947.2s 1302 [GPU 1]
20947.2s 1303 [GPU 1] ======================================================================
20947.2s 1304 [GPU 1]   Dataset: CICIDS2017 | Model: TCN | Seed: 1024
20947.2s 1305 [GPU 1] ======================================================================
20947.2s 1306 [GPU 1] [Reproducibility] Setting all random seeds to 1024
20947.2s 1307 [GPU 1] [Reproducibility] [OK] All seeds set and deterministic mode enabled
20947.2s 1308 [GPU 1] [1/5] Loading CICIDS2017 dataset...
20947.2s 1309 [GPU 1] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
20948.3s 1310 [GPU 1]   Cached shape: (863214, 77), labels: 863214
20948.3s 1311 [GPU 1] [TRAIN] Raw shape (before windowing): (863214, 77)
20948.3s 1312 [GPU 1] Fitting StandardScaler on Train split...
20949.3s 1313 [GPU 1] Saved scaler to outputs/scaler_cicids.pkl
20949.3s 1314 [GPU 1] [OK] Will generate 863165 windows lazily during training
20949.3s 1315 [GPU 1] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
20949.3s 1316 [GPU 1]   Cached shape: (123316, 77), labels: 123316
20949.3s 1317 [GPU 1] [VAL] Raw shape (before windowing): (123316, 77)
20949.4s 1318 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
20949.4s 1319 [GPU 1] [OK] Will generate 123267 windows lazily during training
20949.4s 1320 [GPU 1] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
20949.7s 1321 [GPU 1]   Cached shape: (246633, 77), labels: 246633
20949.7s 1322 [GPU 1] [TEST] Raw shape (before windowing): (246633, 77)
20949.7s 1323 [GPU 1] Loading scaler from outputs/scaler_cicids.pkl...
20949.8s 1324 [GPU 1] [OK] Will generate 246584 windows lazily during training
20949.8s 1325 [GPU 1]   Features: 77, Train: 863165, Val: 123267, Test: 246584
20949.8s 1326 [GPU 1] [2/5] Initializing TCN (d_model=128, n_layers=2)...
20949.8s 1327 [GPU 1]   Parameters: 423,169
20949.9s 1328 [GPU 1] [3/5] Training TCN...
20949.9s 1329 [GPU 1]   LR: 0.000500 (base=0.001 Ã— 0.5 for TCN), grad_clip=1.0
20949.9s 1330 [GPU 1]   Using class-weighted loss (pos_weight=1.00)
21066.4s 1331 [GPU 1]   Epoch   1/30 | TrLoss: 0.0347 | VlLoss: 0.2175 | VlAcc: 0.8837 | VlF1: 0.9366 *
21100.7s 1332 [GPU 0]   Epoch   3/30 | TrLoss: 0.0148 | VlLoss: 0.0189 | VlAcc: 0.9963 | VlF1: 0.9981 *
21182.8s 1333 [GPU 1]   Epoch   2/30 | TrLoss: 0.0228 | VlLoss: 0.0516 | VlAcc: 0.9772 | VlF1: 0.9883 *
21298.6s 1334 [GPU 1]   Epoch   3/30 | TrLoss: 0.0192 | VlLoss: 0.0332 | VlAcc: 0.9905 | VlF1: 0.9951 *
21414.3s 1335 [GPU 1]   Epoch   4/30 | TrLoss: 0.0079 | VlLoss: 0.0420 | VlAcc: 0.9899 | VlF1: 0.9948
21530.2s 1336 [GPU 1]   Epoch   5/30 | TrLoss: 0.0040 | VlLoss: 0.0509 | VlAcc: 0.9825 | VlF1: 0.9910
21645.9s 1337 [GPU 1]   Epoch   6/30 | TrLoss: 0.0031 | VlLoss: 0.0666 | VlAcc: 0.9785 | VlF1: 0.9890
21761.5s 1338 [GPU 1]   Epoch   7/30 | TrLoss: 0.0023 | VlLoss: 0.0657 | VlAcc: 0.9815 | VlF1: 0.9905
21767.5s 1339 [GPU 0]   Epoch   4/30 | TrLoss: 0.0050 | VlLoss: 0.0150 | VlAcc: 0.9969 | VlF1: 0.9984 *
21877.0s 1340 [GPU 1]   Epoch   8/30 | TrLoss: 0.0018 | VlLoss: 0.0647 | VlAcc: 0.9822 | VlF1: 0.9908
21877.0s 1341 [GPU 1]   Early stopping at epoch 8 (patience=5)
21877.0s 1342 [GPU 1]   Best Val F1: 0.9951 (8 epochs, 927.0s)
21877.0s 1343 [GPU 1]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_TCN_seed1024.pt
21877.0s 1344 [GPU 1] [4/5] Evaluating on test set...
21888.1s 1345 [GPU 1]   Test Acc: 0.6877 | Prec: 0.2185 | Rec: 0.9680 | F1: 0.3565 | AUC: 0.9720
21888.1s 1346 [GPU 1] [5/5] Measuring efficiency...
21888.4s 1347 [GPU 1]   Latency: 1.61ms | Throughput: 17847/s | Memory: 1.61MB
21899.1s 1348 [GPU 1]   Attack types analyzed: 3
21899.1s 1349 [GPU 1]
21899.1s 1350 [GPU 1]
21899.1s 1351 [GPU 1] ======================================================================
21899.1s 1352 [GPU 1] BENCHMARK SUMMARY
21899.1s 1353 [GPU 1] ======================================================================
21899.1s 1354 [GPU 1]
21899.1s 1355 [GPU 1] ðŸ“Š CICIDS2017
21899.1s 1356 [GPU 1] Model                Acc     Prec      Rec       F1      AUC  Lat(ms)     Params
21899.1s 1357 [GPU 1] -----------------------------------------------------------------------------------
21899.1s 1358 [GPU 1] Mamba           0.9805  0.8822  0.9195  0.8972  0.9808   17.73     387,073
21899.1s 1359 [GPU 1] LSTM            0.0893  0.0893  1.0000  0.1640  0.5626    0.79     423,169
21899.1s 1360 [GPU 1] GRU             0.5405  0.5394  0.9576  0.5574  0.8573    0.56     373,505
21899.1s 1361 [GPU 1] Transformer     0.1066  0.0906  0.9955  0.1661  0.9079    1.25     406,913
21899.1s 1362 [GPU 1] CNN-LSTM        0.9742  0.8082  0.9328  0.8660  0.9751    0.88     409,601
21899.1s 1363 [GPU 1] TCN             0.7935  0.3434  0.9592  0.4920  0.9702    1.92     423,169
21899.1s 1364 [GPU 1]
21899.1s 1365 [GPU 1] âœ… Results saved to: outputs/benchmark_results/
21899.1s 1366 [GPU 1]    Run 'python generate_tables.py' to generate publication tables.
22424.7s 1367 [GPU 0]   Epoch   5/30 | TrLoss: 0.0037 | VlLoss: 0.0152 | VlAcc: 0.9971 | VlF1: 0.9985 *
23079.9s 1368 [GPU 0]   Epoch   6/30 | TrLoss: 0.0032 | VlLoss: 0.0169 | VlAcc: 0.9963 | VlF1: 0.9981
23736.3s 1369 [GPU 0]   Epoch   7/30 | TrLoss: 0.0024 | VlLoss: 0.0164 | VlAcc: 0.9960 | VlF1: 0.9979
24391.3s 1370 [GPU 0]   Epoch   8/30 | TrLoss: 0.0017 | VlLoss: 0.0373 | VlAcc: 0.9880 | VlF1: 0.9937
25045.3s 1371 [GPU 0]   Epoch   9/30 | TrLoss: 0.0015 | VlLoss: 0.1434 | VlAcc: 0.9733 | VlF1: 0.9860
25699.6s 1372 [GPU 0]   Epoch  10/30 | TrLoss: 0.0018 | VlLoss: 0.0234 | VlAcc: 0.9956 | VlF1: 0.9977
25699.6s 1373 [GPU 0]   Early stopping at epoch 10 (patience=5)
25699.6s 1374 [GPU 0]   Best Val F1: 0.9985 (10 epochs, 6650.0s)
25699.6s 1375 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Mamba_seed456.pt
25699.6s 1376 [GPU 0] [4/5] Evaluating on test set...
25742.5s 1377 [GPU 0]   Test Acc: 0.9904 | Prec: 0.9776 | Rec: 0.9140 | F1: 0.9447 | AUC: 0.9844
25742.5s 1378 [GPU 0] [5/5] Measuring efficiency...
25745.6s 1379 [GPU 0]   Latency: 14.65ms | Throughput: 2149/s | Memory: 1.48MB
25788.2s 1380 [GPU 0]   Attack types analyzed: 3
25788.2s 1381 [GPU 0]
25788.2s 1382 [GPU 0] [14/18] Running experiment...
25788.2s 1383 [GPU 0]
25788.2s 1384 [GPU 0] ======================================================================
25788.2s 1385 [GPU 0]   Dataset: CICIDS2017 | Model: LSTM | Seed: 456
25788.2s 1386 [GPU 0] ======================================================================
25788.2s 1387 [GPU 0] [Reproducibility] Setting all random seeds to 456
25788.2s 1388 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
25788.2s 1389 [GPU 0] [1/5] Loading CICIDS2017 dataset...
25788.2s 1390 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
25789.3s 1391 [GPU 0]   Cached shape: (863214, 77), labels: 863214
25789.3s 1392 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
25789.3s 1393 [GPU 0] Fitting StandardScaler on Train split...
25790.2s 1394 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
25790.2s 1395 [GPU 0] [OK] Will generate 863165 windows lazily during training
25790.2s 1396 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
25790.3s 1397 [GPU 0]   Cached shape: (123316, 77), labels: 123316
25790.3s 1398 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
25790.3s 1399 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
25790.3s 1400 [GPU 0] [OK] Will generate 123267 windows lazily during training
25790.3s 1401 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
25790.6s 1402 [GPU 0]   Cached shape: (246633, 77), labels: 246633
25790.6s 1403 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
25790.6s 1404 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
25790.7s 1405 [GPU 0] [OK] Will generate 246584 windows lazily during training
25790.7s 1406 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
25790.7s 1407 [GPU 0] [2/5] Initializing LSTM (d_model=128, n_layers=2)...
25790.7s 1408 [GPU 0]   Parameters: 423,169
25790.8s 1409 [GPU 0] [3/5] Training LSTM...
25790.8s 1410 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for LSTM), grad_clip=1.0
25790.8s 1411 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
25878.4s 1412 [GPU 0]   Epoch   1/30 | TrLoss: 0.0307 | VlLoss: 0.1652 | VlAcc: 0.9642 | VlF1: 0.9818 *
25965.2s 1413 [GPU 0]   Epoch   2/30 | TrLoss: 0.0331 | VlLoss: 0.1827 | VlAcc: 0.9642 | VlF1: 0.9818
26052.3s 1414 [GPU 0]   Epoch   3/30 | TrLoss: 0.0365 | VlLoss: 0.1825 | VlAcc: 0.9642 | VlF1: 0.9818
26139.3s 1415 [GPU 0]   Epoch   4/30 | TrLoss: 0.0422 | VlLoss: 0.1770 | VlAcc: 0.9642 | VlF1: 0.9818
26226.6s 1416 [GPU 0]   Epoch   5/30 | TrLoss: 0.0434 | VlLoss: 0.1865 | VlAcc: 0.9642 | VlF1: 0.9818
26313.4s 1417 [GPU 0]   Epoch   6/30 | TrLoss: 0.0406 | VlLoss: 0.1381 | VlAcc: 0.9707 | VlF1: 0.9850 *
26400.5s 1418 [GPU 0]   Epoch   7/30 | TrLoss: 0.0080 | VlLoss: 0.0220 | VlAcc: 0.9954 | VlF1: 0.9976 *
26487.4s 1419 [GPU 0]   Epoch   8/30 | TrLoss: 0.0041 | VlLoss: 0.0168 | VlAcc: 0.9963 | VlF1: 0.9981 *
26574.3s 1420 [GPU 0]   Epoch   9/30 | TrLoss: 0.0017 | VlLoss: 0.0162 | VlAcc: 0.9967 | VlF1: 0.9983 *
26661.2s 1421 [GPU 0]   Epoch  10/30 | TrLoss: 0.0013 | VlLoss: 0.0172 | VlAcc: 0.9968 | VlF1: 0.9983 *
26748.3s 1422 [GPU 0]   Epoch  11/30 | TrLoss: 0.0010 | VlLoss: 0.0151 | VlAcc: 0.9970 | VlF1: 0.9984 *
26835.3s 1423 [GPU 0]   Epoch  12/30 | TrLoss: 0.0008 | VlLoss: 0.0160 | VlAcc: 0.9969 | VlF1: 0.9984
26922.1s 1424 [GPU 0]   Epoch  13/30 | TrLoss: 0.0007 | VlLoss: 0.0154 | VlAcc: 0.9972 | VlF1: 0.9986 *
27009.2s 1425 [GPU 0]   Epoch  14/30 | TrLoss: 0.0006 | VlLoss: 0.0156 | VlAcc: 0.9962 | VlF1: 0.9980
27096.1s 1426 [GPU 0]   Epoch  15/30 | TrLoss: 0.0008 | VlLoss: 0.0145 | VlAcc: 0.9970 | VlF1: 0.9984
27183.2s 1427 [GPU 0]   Epoch  16/30 | TrLoss: 0.0006 | VlLoss: 0.0168 | VlAcc: 0.9966 | VlF1: 0.9982
27270.2s 1428 [GPU 0]   Epoch  17/30 | TrLoss: 0.0004 | VlLoss: 0.0211 | VlAcc: 0.9967 | VlF1: 0.9983
27357.2s 1429 [GPU 0]   Epoch  18/30 | TrLoss: 0.0004 | VlLoss: 0.0237 | VlAcc: 0.9966 | VlF1: 0.9982
27357.2s 1430 [GPU 0]   Early stopping at epoch 18 (patience=5)
27357.2s 1431 [GPU 0]   Best Val F1: 0.9986 (18 epochs, 1566.4s)
27357.2s 1432 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_LSTM_seed456.pt
27357.2s 1433 [GPU 0] [4/5] Evaluating on test set...
27369.7s 1434 [GPU 0]   Test Acc: 0.9911 | Prec: 0.9962 | Rec: 0.9044 | F1: 0.9480 | AUC: 0.9827
27369.7s 1435 [GPU 0] [5/5] Measuring efficiency...
27369.9s 1436 [GPU 0]   Latency: 0.76ms | Throughput: 30553/s | Memory: 1.61MB
27382.3s 1437 [GPU 0]   Attack types analyzed: 3
27382.3s 1438 [GPU 0]
27382.3s 1439 [GPU 0] [15/18] Running experiment...
27382.3s 1440 [GPU 0]
27382.3s 1441 [GPU 0] ======================================================================
27382.3s 1442 [GPU 0]   Dataset: CICIDS2017 | Model: GRU | Seed: 456
27382.3s 1443 [GPU 0] ======================================================================
27382.3s 1444 [GPU 0] [Reproducibility] Setting all random seeds to 456
27382.3s 1445 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
27382.3s 1446 [GPU 0] [1/5] Loading CICIDS2017 dataset...
27382.3s 1447 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
27383.5s 1448 [GPU 0]   Cached shape: (863214, 77), labels: 863214
27383.5s 1449 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
27383.5s 1450 [GPU 0] Fitting StandardScaler on Train split...
27384.4s 1451 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
27384.4s 1452 [GPU 0] [OK] Will generate 863165 windows lazily during training
27384.4s 1453 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
27384.5s 1454 [GPU 0]   Cached shape: (123316, 77), labels: 123316
27384.5s 1455 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
27384.5s 1456 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
27384.5s 1457 [GPU 0] [OK] Will generate 123267 windows lazily during training
27384.5s 1458 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
27384.8s 1459 [GPU 0]   Cached shape: (246633, 77), labels: 246633
27384.8s 1460 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
27384.8s 1461 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
27384.9s 1462 [GPU 0] [OK] Will generate 246584 windows lazily during training
27384.9s 1463 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
27384.9s 1464 [GPU 0] [2/5] Initializing GRU (d_model=128, n_layers=2)...
27384.9s 1465 [GPU 0]   Parameters: 373,505
27385.0s 1466 [GPU 0] [3/5] Training GRU...
27385.0s 1467 [GPU 0]   LR: 0.001000 (base=0.001 Ã— 1.0 for GRU), grad_clip=1.0
27385.0s 1468 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
27458.5s 1469 [GPU 0]   Epoch   1/30 | TrLoss: 0.0700 | VlLoss: 0.1849 | VlAcc: 0.9642 | VlF1: 0.9818 *
27531.9s 1470 [GPU 0]   Epoch   2/30 | TrLoss: 0.1349 | VlLoss: 0.1879 | VlAcc: 0.9642 | VlF1: 0.9818
27605.1s 1471 [GPU 0]   Epoch   3/30 | TrLoss: 0.0437 | VlLoss: 0.1825 | VlAcc: 0.9642 | VlF1: 0.9818
27678.2s 1472 [GPU 0]   Epoch   4/30 | TrLoss: 0.0376 | VlLoss: 0.1771 | VlAcc: 0.9642 | VlF1: 0.9818
27751.2s 1473 [GPU 0]   Epoch   5/30 | TrLoss: 0.0400 | VlLoss: 0.1779 | VlAcc: 0.9642 | VlF1: 0.9818
27824.3s 1474 [GPU 0]   Epoch   6/30 | TrLoss: 0.0265 | VlLoss: 0.0988 | VlAcc: 0.9692 | VlF1: 0.9843 *
27897.3s 1475 [GPU 0]   Epoch   7/30 | TrLoss: 0.0136 | VlLoss: 0.1061 | VlAcc: 0.9757 | VlF1: 0.9876 *
27970.2s 1476 [GPU 0]   Epoch   8/30 | TrLoss: 0.0084 | VlLoss: 0.0940 | VlAcc: 0.9780 | VlF1: 0.9887 *
28043.1s 1477 [GPU 0]   Epoch   9/30 | TrLoss: 0.0053 | VlLoss: 1.2659 | VlAcc: 0.5725 | VlF1: 0.7163
28116.1s 1478 [GPU 0]   Epoch  10/30 | TrLoss: 0.0036 | VlLoss: 1.6794 | VlAcc: 0.4435 | VlF1: 0.5949
28188.9s 1479 [GPU 0]   Epoch  11/30 | TrLoss: 0.0016 | VlLoss: 0.0411 | VlAcc: 0.9892 | VlF1: 0.9944 *
28261.9s 1480 [GPU 0]   Epoch  12/30 | TrLoss: 0.0015 | VlLoss: 0.0493 | VlAcc: 0.9846 | VlF1: 0.9920
28335.0s 1481 [GPU 0]   Epoch  13/30 | TrLoss: 0.0013 | VlLoss: 0.2201 | VlAcc: 0.8817 | VlF1: 0.9347
28408.0s 1482 [GPU 0]   Epoch  14/30 | TrLoss: 0.0009 | VlLoss: 0.1056 | VlAcc: 0.9571 | VlF1: 0.9773
28480.9s 1483 [GPU 0]   Epoch  15/30 | TrLoss: 0.0007 | VlLoss: 0.0436 | VlAcc: 0.9923 | VlF1: 0.9960 *
28554.0s 1484 [GPU 0]   Epoch  16/30 | TrLoss: 0.0005 | VlLoss: 0.0524 | VlAcc: 0.9907 | VlF1: 0.9951
28626.8s 1485 [GPU 0]   Epoch  17/30 | TrLoss: 0.0005 | VlLoss: 0.1201 | VlAcc: 0.9716 | VlF1: 0.9851
28699.7s 1486 [GPU 0]   Epoch  18/30 | TrLoss: 0.0004 | VlLoss: 0.1156 | VlAcc: 0.9741 | VlF1: 0.9864
28772.9s 1487 [GPU 0]   Epoch  19/30 | TrLoss: 0.0003 | VlLoss: 0.1107 | VlAcc: 0.9822 | VlF1: 0.9907
28845.9s 1488 [GPU 0]   Epoch  20/30 | TrLoss: 0.0003 | VlLoss: 0.0687 | VlAcc: 0.9903 | VlF1: 0.9949
28845.9s 1489 [GPU 0]   Early stopping at epoch 20 (patience=5)
28845.9s 1490 [GPU 0]   Best Val F1: 0.9960 (20 epochs, 1460.9s)
28845.9s 1491 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_GRU_seed456.pt
28845.9s 1492 [GPU 0] [4/5] Evaluating on test set...
28856.7s 1493 [GPU 0]   Test Acc: 0.9912 | Prec: 0.9959 | Rec: 0.9050 | F1: 0.9483 | AUC: 0.9900
28856.7s 1494 [GPU 0] [5/5] Measuring efficiency...
28856.8s 1495 [GPU 0]   Latency: 0.49ms | Throughput: 43990/s | Memory: 1.42MB
28867.5s 1496 [GPU 0]   Attack types analyzed: 3
28867.5s 1497 [GPU 0]
28867.5s 1498 [GPU 0] [16/18] Running experiment...
28867.5s 1499 [GPU 0]
28867.5s 1500 [GPU 0] ======================================================================
28867.5s 1501 [GPU 0]   Dataset: CICIDS2017 | Model: Transformer | Seed: 456
28867.5s 1502 [GPU 0] ======================================================================
28867.5s 1503 [GPU 0] [Reproducibility] Setting all random seeds to 456
28867.5s 1504 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
28867.5s 1505 [GPU 0] [1/5] Loading CICIDS2017 dataset...
28867.5s 1506 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
28868.5s 1507 [GPU 0]   Cached shape: (863214, 77), labels: 863214
28868.5s 1508 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
28868.5s 1509 [GPU 0] Fitting StandardScaler on Train split...
28869.5s 1510 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
28869.5s 1511 [GPU 0] [OK] Will generate 863165 windows lazily during training
28869.5s 1512 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
28869.6s 1513 [GPU 0]   Cached shape: (123316, 77), labels: 123316
28869.6s 1514 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
28869.6s 1515 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
28869.6s 1516 [GPU 0] [OK] Will generate 123267 windows lazily during training
28869.6s 1517 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
28869.9s 1518 [GPU 0]   Cached shape: (246633, 77), labels: 246633
28869.9s 1519 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
28869.9s 1520 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
28870.0s 1521 [GPU 0] [OK] Will generate 246584 windows lazily during training
28870.0s 1522 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
28870.0s 1523 [GPU 0] [2/5] Initializing Transformer (d_model=128, n_layers=2)...
28870.0s 1524 [GPU 0]   Parameters: 406,913
28870.1s 1525 [GPU 0] [3/5] Training Transformer...
28870.1s 1526 [GPU 0]   LR: 0.000300 (base=0.001 Ã— 0.3 for Transformer), grad_clip=0.5
28870.1s 1527 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
28972.3s 1528 [GPU 0]   Epoch   1/30 | TrLoss: 0.0739 | VlLoss: 0.0892 | VlAcc: 0.9748 | VlF1: 0.9871 *
29074.4s 1529 [GPU 0]   Epoch   2/30 | TrLoss: 0.0263 | VlLoss: 0.1041 | VlAcc: 0.9727 | VlF1: 0.9860
29176.9s 1530 [GPU 0]   Epoch   3/30 | TrLoss: 0.0428 | VlLoss: 0.1440 | VlAcc: 0.9642 | VlF1: 0.9818
29279.2s 1531 [GPU 0]   Epoch   4/30 | TrLoss: 0.0317 | VlLoss: 0.5090 | VlAcc: 0.9009 | VlF1: 0.9467
29381.3s 1532 [GPU 0]   Epoch   5/30 | TrLoss: 0.0245 | VlLoss: 0.5694 | VlAcc: 0.8881 | VlF1: 0.9394
29483.6s 1533 [GPU 0]   Epoch   6/30 | TrLoss: 0.0098 | VlLoss: 3.9530 | VlAcc: 0.3323 | VlF1: 0.4708
29483.6s 1534 [GPU 0]   Early stopping at epoch 6 (patience=5)
29483.6s 1535 [GPU 0]   Best Val F1: 0.9871 (6 epochs, 613.4s)
29483.6s 1536 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_Transformer_seed456.pt
29483.6s 1537 [GPU 0] [4/5] Evaluating on test set...
29494.8s 1538 [GPU 0]   Test Acc: 0.1441 | Prec: 0.0945 | Rec: 0.9991 | F1: 0.1726 | AUC: 0.9583
29494.8s 1539 [GPU 0] [5/5] Measuring efficiency...
29495.1s 1540 [GPU 0]   Latency: 1.26ms | Throughput: 27149/s | Memory: 1.55MB
29506.2s 1541 [GPU 0]   Attack types analyzed: 3
29506.2s 1542 [GPU 0]
29506.2s 1543 [GPU 0] [17/18] Running experiment...
29506.2s 1544 [GPU 0]
29506.2s 1545 [GPU 0] ======================================================================
29506.2s 1546 [GPU 0]   Dataset: CICIDS2017 | Model: CNN-LSTM | Seed: 456
29506.2s 1547 [GPU 0] ======================================================================
29506.2s 1548 [GPU 0] [Reproducibility] Setting all random seeds to 456
29506.2s 1549 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
29506.2s 1550 [GPU 0] [1/5] Loading CICIDS2017 dataset...
29506.2s 1551 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
29507.3s 1552 [GPU 0]   Cached shape: (863214, 77), labels: 863214
29507.3s 1553 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
29507.3s 1554 [GPU 0] Fitting StandardScaler on Train split...
29508.2s 1555 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
29508.2s 1556 [GPU 0] [OK] Will generate 863165 windows lazily during training
29508.2s 1557 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
29508.3s 1558 [GPU 0]   Cached shape: (123316, 77), labels: 123316
29508.3s 1559 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
29508.3s 1560 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
29508.3s 1561 [GPU 0] [OK] Will generate 123267 windows lazily during training
29508.3s 1562 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
29508.6s 1563 [GPU 0]   Cached shape: (246633, 77), labels: 246633
29508.6s 1564 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
29508.6s 1565 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
29508.7s 1566 [GPU 0] [OK] Will generate 246584 windows lazily during training
29508.7s 1567 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
29508.7s 1568 [GPU 0] [2/5] Initializing CNN-LSTM (d_model=128, n_layers=2)...
29508.7s 1569 [GPU 0]   Parameters: 409,601
29508.8s 1570 [GPU 0] [3/5] Training CNN-LSTM...
29508.8s 1571 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for CNN-LSTM), grad_clip=0.5
29508.8s 1572 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
29595.8s 1573 [GPU 0]   Epoch   1/30 | TrLoss: 0.0909 | VlLoss: 0.1791 | VlAcc: 0.9642 | VlF1: 0.9818 *
29682.4s 1574 [GPU 0]   Epoch   2/30 | TrLoss: 0.0477 | VlLoss: 0.1471 | VlAcc: 0.9816 | VlF1: 0.9905 *
29768.9s 1575 [GPU 0]   Epoch   3/30 | TrLoss: 0.0401 | VlLoss: 0.7822 | VlAcc: 0.3608 | VlF1: 0.5133
29855.6s 1576 [GPU 0]   Epoch   4/30 | TrLoss: 0.0449 | VlLoss: 0.1466 | VlAcc: 0.9728 | VlF1: 0.9859
29942.3s 1577 [GPU 0]   Epoch   5/30 | TrLoss: 0.0570 | VlLoss: 0.1749 | VlAcc: 0.9642 | VlF1: 0.9818
30029.0s 1578 [GPU 0]   Epoch   6/30 | TrLoss: 0.0284 | VlLoss: 0.8538 | VlAcc: 0.3898 | VlF1: 0.5396
30115.5s 1579 [GPU 0]   Epoch   7/30 | TrLoss: 0.0035 | VlLoss: 0.1066 | VlAcc: 0.9913 | VlF1: 0.9955 *
30202.0s 1580 [GPU 0]   Epoch   8/30 | TrLoss: 0.0026 | VlLoss: 0.1518 | VlAcc: 0.9919 | VlF1: 0.9958 *
30288.8s 1581 [GPU 0]   Epoch   9/30 | TrLoss: 0.0023 | VlLoss: 0.0365 | VlAcc: 0.9938 | VlF1: 0.9968 *
30375.6s 1582 [GPU 0]   Epoch  10/30 | TrLoss: 0.0018 | VlLoss: 0.0318 | VlAcc: 0.9945 | VlF1: 0.9971 *
30462.3s 1583 [GPU 0]   Epoch  11/30 | TrLoss: 0.0016 | VlLoss: 0.0284 | VlAcc: 0.9954 | VlF1: 0.9976 *
30548.9s 1584 [GPU 0]   Epoch  12/30 | TrLoss: 0.0015 | VlLoss: 0.0246 | VlAcc: 0.9961 | VlF1: 0.9980 *
30635.6s 1585 [GPU 0]   Epoch  13/30 | TrLoss: 0.0014 | VlLoss: 0.0250 | VlAcc: 0.9960 | VlF1: 0.9979
30722.3s 1586 [GPU 0]   Epoch  14/30 | TrLoss: 0.0010 | VlLoss: 0.0269 | VlAcc: 0.9964 | VlF1: 0.9981 *
30808.8s 1587 [GPU 0]   Epoch  15/30 | TrLoss: 0.0009 | VlLoss: 0.0296 | VlAcc: 0.9960 | VlF1: 0.9979
30895.7s 1588 [GPU 0]   Epoch  16/30 | TrLoss: 0.0008 | VlLoss: 0.0245 | VlAcc: 0.9959 | VlF1: 0.9979
30982.3s 1589 [GPU 0]   Epoch  17/30 | TrLoss: 0.0009 | VlLoss: 0.0262 | VlAcc: 0.9942 | VlF1: 0.9970
31068.8s 1590 [GPU 0]   Epoch  18/30 | TrLoss: 0.0005 | VlLoss: 0.0260 | VlAcc: 0.9963 | VlF1: 0.9981
31155.3s 1591 [GPU 0]   Epoch  19/30 | TrLoss: 0.0004 | VlLoss: 0.0291 | VlAcc: 0.9964 | VlF1: 0.9981
31155.3s 1592 [GPU 0]   Early stopping at epoch 19 (patience=5)
31155.3s 1593 [GPU 0]   Best Val F1: 0.9981 (19 epochs, 1646.5s)
31155.3s 1594 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_CNN-LSTM_seed456.pt
31155.3s 1595 [GPU 0] [4/5] Evaluating on test set...
31166.4s 1596 [GPU 0]   Test Acc: 0.9643 | Prec: 0.7350 | Rec: 0.9386 | F1: 0.8244 | AUC: 0.9718
31166.4s 1597 [GPU 0] [5/5] Measuring efficiency...
31166.6s 1598 [GPU 0]   Latency: 0.88ms | Throughput: 34323/s | Memory: 1.56MB
31177.8s 1599 [GPU 0]   Attack types analyzed: 3
31177.8s 1600 [GPU 0]
31177.8s 1601 [GPU 0] [18/18] Running experiment...
31177.8s 1602 [GPU 0]
31177.8s 1603 [GPU 0] ======================================================================
31177.8s 1604 [GPU 0]   Dataset: CICIDS2017 | Model: TCN | Seed: 456
31177.8s 1605 [GPU 0] ======================================================================
31177.8s 1606 [GPU 0] [Reproducibility] Setting all random seeds to 456
31177.8s 1607 [GPU 0] [Reproducibility] [OK] All seeds set and deterministic mode enabled
31177.8s 1608 [GPU 0] [1/5] Loading CICIDS2017 dataset...
31177.8s 1609 [GPU 0] [TRAIN] Loading from cache: outputs/cache/cicids2017_train.npz
31178.9s 1610 [GPU 0]   Cached shape: (863214, 77), labels: 863214
31178.9s 1611 [GPU 0] [TRAIN] Raw shape (before windowing): (863214, 77)
31178.9s 1612 [GPU 0] Fitting StandardScaler on Train split...
31179.8s 1613 [GPU 0] Saved scaler to outputs/scaler_cicids.pkl
31179.8s 1614 [GPU 0] [OK] Will generate 863165 windows lazily during training
31179.8s 1615 [GPU 0] [VAL] Loading from cache: outputs/cache/cicids2017_val.npz
31179.9s 1616 [GPU 0]   Cached shape: (123316, 77), labels: 123316
31179.9s 1617 [GPU 0] [VAL] Raw shape (before windowing): (123316, 77)
31179.9s 1618 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
31179.9s 1619 [GPU 0] [OK] Will generate 123267 windows lazily during training
31179.9s 1620 [GPU 0] [TEST] Loading from cache: outputs/cache/cicids2017_test.npz
31180.3s 1621 [GPU 0]   Cached shape: (246633, 77), labels: 246633
31180.3s 1622 [GPU 0] [TEST] Raw shape (before windowing): (246633, 77)
31180.3s 1623 [GPU 0] Loading scaler from outputs/scaler_cicids.pkl...
31180.4s 1624 [GPU 0] [OK] Will generate 246584 windows lazily during training
31180.4s 1625 [GPU 0]   Features: 77, Train: 863165, Val: 123267, Test: 246584
31180.4s 1626 [GPU 0] [2/5] Initializing TCN (d_model=128, n_layers=2)...
31180.4s 1627 [GPU 0]   Parameters: 423,169
31180.4s 1628 [GPU 0] [3/5] Training TCN...
31180.4s 1629 [GPU 0]   LR: 0.000500 (base=0.001 Ã— 0.5 for TCN), grad_clip=1.0
31180.4s 1630 [GPU 0]   Using class-weighted loss (pos_weight=1.00)
31289.3s 1631 [GPU 0]   Epoch   1/30 | TrLoss: 0.0410 | VlLoss: 0.0465 | VlAcc: 0.9825 | VlF1: 0.9910 *
31397.9s 1632 [GPU 0]   Epoch   2/30 | TrLoss: 0.0105 | VlLoss: 0.0885 | VlAcc: 0.9731 | VlF1: 0.9862
31506.6s 1633 [GPU 0]   Epoch   3/30 | TrLoss: 0.0174 | VlLoss: 0.0721 | VlAcc: 0.9757 | VlF1: 0.9875
31615.2s 1634 [GPU 0]   Epoch   4/30 | TrLoss: 0.0076 | VlLoss: 0.1476 | VlAcc: 0.9806 | VlF1: 0.9898
31723.7s 1635 [GPU 0]   Epoch   5/30 | TrLoss: 0.0035 | VlLoss: 0.0631 | VlAcc: 0.9787 | VlF1: 0.9891
31832.3s 1636 [GPU 0]   Epoch   6/30 | TrLoss: 0.0023 | VlLoss: 0.0605 | VlAcc: 0.9800 | VlF1: 0.9897
31832.3s 1637 [GPU 0]   Early stopping at epoch 6 (patience=5)
31832.3s 1638 [GPU 0]   Best Val F1: 0.9910 (6 epochs, 651.9s)
31832.3s 1639 [GPU 0]   ðŸ’¾ Model saved to: outputs/checkpoints/CICIDS2017_TCN_seed456.pt
31832.3s 1640 [GPU 0] [4/5] Evaluating on test set...
31843.0s 1641 [GPU 0]   Test Acc: 0.7790 | Prec: 0.2834 | Rec: 0.9640 | F1: 0.4380 | AUC: 0.9749
31843.0s 1642 [GPU 0] [5/5] Measuring efficiency...
31843.3s 1643 [GPU 0]   Latency: 1.63ms | Throughput: 17465/s | Memory: 1.61MB
31853.9s 1644 [GPU 0]   Attack types analyzed: 3
31853.9s 1645 [GPU 0]
31853.9s 1646 [GPU 0]
31853.9s 1647 [GPU 0] ======================================================================
31853.9s 1648 [GPU 0] BENCHMARK SUMMARY
31853.9s 1649 [GPU 0] ======================================================================
31853.9s 1650 [GPU 0]
31853.9s 1651 [GPU 0] ðŸ“Š CICIDS2017
31853.9s 1652 [GPU 0] Model                Acc     Prec      Rec       F1      AUC  Lat(ms)     Params
31853.9s 1653 [GPU 0] -----------------------------------------------------------------------------------
31853.9s 1654 [GPU 0] Mamba           0.9815  0.8726  0.9414  0.9029  0.9830   15.53     387,073
31853.9s 1655 [GPU 0] LSTM            0.6850  0.6271  0.9552  0.6609  0.8231    0.76     423,169
31853.9s 1656 [GPU 0] GRU             0.3900  0.3915  0.9683  0.4255  0.7518    0.52     373,505
31853.9s 1657 [GPU 0] Transformer     0.1077  0.0910  0.9994  0.1669  0.6991    1.33     406,913
31853.9s 1658 [GPU 0] CNN-LSTM        0.9645  0.7360  0.9460  0.8271  0.9736    0.96     409,601
31853.9s 1659 [GPU 0] TCN             0.7223  0.2902  0.9648  0.4325  0.9710    1.73     423,169
31853.9s 1660 [GPU 0]
31853.9s 1661 [GPU 0] âœ… Results saved to: outputs/benchmark_results/
31853.9s 1662 [GPU 0]    Run 'python generate_tables.py' to generate publication tables.
31854.8s 1663 
31854.8s 1664 âœ… GPU 0 finished (exit=0)
31854.8s 1665 âœ… GPU 1 finished (exit=0)
31854.8s 1666 â±ï¸  Total time: 8h 50m
31854.8s 1667 
31854.8s 1668 ============================================================
31854.8s 1669 ðŸ All done! Total time: 8h 50m
31854.8s 1670 ============================================================
31854.8s 1671 
31854.8s 1672 ðŸ“Š Merging results from both GPUs...
31854.8s 1673 âœ… Merged 30 experiments from 30 files into all_results.json
31854.8s 1674 ðŸ“‹ Generating publication tables...
31857.5s 1675 Loaded 30 experiment results.
31857.5s 1676 
31857.5s 1677 ## Table 1: Classification Results â€” CICIDS2017
31857.5s 1678 *Mean Â± std across 5 seeds*
31857.5s 1679 
31857.5s 1680 | Model | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
31857.5s 1681 |-------|----------|-----------|--------|----------|---------|
31857.5s 1682 | Mamba | 98.11 Â± 0.88 | 87.64 Â± 8.98 | 93.27 Â± 1.87 | 90.06 Â± 4.18 | 98.21 Â± 0.20 |
31857.5s 1683 | CNN-LSTM | 96.84 Â± 0.68 | 76.49 Â± 4.83 | 94.07 Â± 0.89 | 84.26 Â± 2.69 | 97.42 Â± 0.20 |
31857.5s 1684 | GRU | 45.02 Â± 44.18 | 45.07 Â± 44.25 | 96.41 Â± 4.41 | 47.83 Â± 38.48 | 79.40 Â± 17.95 |
31857.5s 1685 | LSTM | 44.68 Â± 43.77 | 41.20 Â± 40.02 | 97.31 Â± 3.75 | 46.22 Â± 36.60 | 71.89 Â± 23.15 |
31857.5s 1686 | TCN | 75.08 Â± 14.66 | 31.15 Â± 12.36 | 96.26 Â± 0.87 | 45.63 Â± 14.10 | 97.07 Â± 0.25 |
31857.5s 1687 | Transformer | 10.73 Â± 2.27 | 9.09 Â± 0.20 | 99.78 Â± 0.35 | 16.66 Â± 0.34 | 78.26 Â± 20.66 |
31857.5s 1688 
31857.5s 1689 ## Table 2: Efficiency Comparison â€” CICIDS2017
31857.5s 1690 
31857.5s 1691 | Model | Params | Memory (MB) | Latency (ms) | Throughput (/s) | Train Time (s) |
31857.5s 1692 |-------|--------|-------------|--------------|-----------------|----------------|
31857.5s 1693 | GRU | 373,505 | 1.42 | 0.54 Â± 0.05 | 42406 | 776.7 |
31857.5s 1694 | LSTM | 423,169 | 1.61 | 0.77 Â± 0.03 | 29679 | 861.9 |
31857.5s 1695 | CNN-LSTM | 409,601 | 1.56 | 0.93 Â± 0.06 | 32864 | 1535.7 |
31857.5s 1696 | Transformer | 406,913 | 1.55 | 1.30 Â± 0.09 | 25627 | 692.5 |
31857.5s 1697 | TCN | 423,169 | 1.61 | 1.81 Â± 0.23 | 16503 | 946.7 |
31857.5s 1698 | Mamba | 387,073 | 1.48 | 16.41 Â± 2.59 | 1963 | 5675.6 |
31857.5s 1699 
31857.5s 1700 ## Table 3: Per-Attack-Type F1 (%) â€” CICIDS2017
31857.5s 1701 
31857.5s 1702 | Model | BENIGN | Bot | PortScan |
31857.5s 1703 |-------|--------|-----|----------|
31857.5s 1704 | CNN-LSTM | 98.5 | 55.0 | 99.8 |
31857.5s 1705 | GRU | 40.0 | 64.6 | 99.9 |
31857.5s 1706 | LSTM | 39.8 | 74.9 | 99.9 |
31857.5s 1707 | Mamba | 99.3 | 46.9 | 99.6 |
31857.5s 1708 | TCN | 83.3 | 75.6 | 99.8 |
31857.5s 1709 | Transformer | 3.8 | 99.4 | 99.9 |
31857.5s 1710 
31857.5s 1711 ## Table 4: Statistical Significance (Wilcoxon) â€” CICIDS2017
31857.5s 1712 
31857.5s 1713 | Model A | Model B | p-value | Significant (p<0.05) |
31857.5s 1714 |---------|---------|---------|----------------------|
31857.5s 1715 | CNN-LSTM | GRU | 0.3125 | âœ— |
31857.5s 1716 | CNN-LSTM | LSTM | 0.3125 | âœ— |
31857.5s 1717 | CNN-LSTM | Mamba | 0.1875 | âœ— |
31857.5s 1718 | CNN-LSTM | TCN | 0.0625 | âœ— |
31857.5s 1719 | CNN-LSTM | Transformer | 0.0625 | âœ— |
31857.5s 1720 | GRU | LSTM | 0.8125 | âœ— |
31857.5s 1721 | GRU | Mamba | 0.3125 | âœ— |
31857.5s 1722 | GRU | TCN | 0.8125 | âœ— |
31857.5s 1723 | GRU | Transformer | 0.1875 | âœ— |
31857.5s 1724 | LSTM | Mamba | 0.1250 | âœ— |
31857.5s 1725 | LSTM | TCN | 1.0000 | âœ— |
31857.5s 1726 | LSTM | Transformer | 0.4375 | âœ— |
31857.5s 1727 | Mamba | TCN | 0.0625 | âœ— |
31857.5s 1728 | Mamba | Transformer | 0.0625 | âœ— |
31857.5s 1729 | TCN | Transformer | 0.0625 | âœ— |
31857.5s 1730 
31857.5s 1731 
31857.5s 1732 âœ… Tables saved to outputs/benchmark_results/benchmark_tables.md
31857.9s 1733 
31857.9s 1734 âœ… Results saved to outputs/benchmark_results/
31860.8s 1735 /usr/local/lib/python3.12/dist-packages/mistune.py:435: SyntaxWarning: invalid escape sequence '\|'
31860.8s 1736 cells[i][c] = re.sub('\\\\\|', '|', cell)
31861.0s 1737 /usr/local/lib/python3.12/dist-packages/nbconvert/filters/filter_links.py:36: SyntaxWarning: invalid escape sequence '\_'
31861.0s 1738 text = re.sub(r'_', '\_', text) # Escape underscores in display text
31861.6s 1739 /usr/local/lib/python3.12/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
31861.6s 1740 warn(
31861.6s 1741 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook
31862.0s 1742 [NbConvertApp] Writing 131210 bytes to __notebook__.ipynb
31864.2s 1743 /usr/local/lib/python3.12/dist-packages/traitlets/traitlets.py:2915: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.
31864.2s 1744 warn(
31864.3s 1745 [NbConvertApp] Converting notebook __notebook__.ipynb to html
31865.1s 1746 [NbConvertApp] Writing 382957 bytes to __results__.html